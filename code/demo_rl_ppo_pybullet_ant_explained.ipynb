{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization on PyBullet Ant explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packaged loaded. TF version is [1.15.0].\n"
     ]
    }
   ],
   "source": [
    "import datetime,gym,os,pybullet_envs,psutil,time,os\n",
    "import scipy.signal\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.set_printoptions(precision=2)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "gym.logger.set_level(40)\n",
    "print (\"Packaged loaded. TF version is [%s].\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def statistics_scalar(x, with_min_and_max=False):\n",
    "    \"\"\"\n",
    "    Get mean/std and optional min/max of scalar x \n",
    "    Args:\n",
    "        x: An array containing samples of the scalar to produce statistics for.\n",
    "        with_min_and_max (bool): If true, return min and max of x in \n",
    "            addition to mean and std.\n",
    "    \"\"\"\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    global_sum, global_n = np.sum(x), len(x)\n",
    "    mean = global_sum / global_n\n",
    "    global_sum_sq = np.sum((x - mean)**2)\n",
    "    std = np.sqrt(global_sum_sq / global_n)  # compute global std\n",
    "    if with_min_and_max:\n",
    "        global_min = (np.min(x) if len(x) > 0 else np.inf)\n",
    "        global_max = (np.max(x) if len(x) > 0 else -np.inf)\n",
    "        return mean, std, global_min, global_max\n",
    "    return mean, std\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    Compute discounted cumulative sums of vectors.\n",
    "    input: \n",
    "        vector x, [x0, x1, x2]\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  \n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "class PPOBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a PPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "    def __init__(self, odim, adim, size=5000, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, odim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, adim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = statistics_scalar(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        return [self.obs_buf, self.act_buf, self.adv_buf, \n",
    "                self.ret_buf, self.logp_buf]\n",
    "    \n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "def create_ppo_model(env=None,hdims=[256,256]):\n",
    "    \"\"\"\n",
    "    Create PPO Actor-Critic Model (compatible with Ray)\n",
    "    \"\"\"\n",
    "    import tensorflow as tf # make it compatible with Ray actors\n",
    "    from gym.spaces import Box, Discrete\n",
    "    \n",
    "    def mlp(x, hdims=[64,64], actv=tf.nn.relu, output_actv=None):\n",
    "        for h in hdims[:-1]:\n",
    "            x = tf.layers.dense(x, units=h, activation=actv)\n",
    "        return tf.layers.dense(x, units=hdims[-1], activation=output_actv)\n",
    "    \n",
    "    def mlp_categorical_policy(o, a, hdims=[64,64], actv=tf.nn.relu, output_actv=None, action_space=None):\n",
    "        adim = action_space.n\n",
    "        logits = mlp(x=o, hdims=hdims+[adim], actv=actv, output_actv=None)\n",
    "        logp_all = tf.nn.log_softmax(logits)\n",
    "        pi = tf.squeeze(tf.multinomial(logits,1), axis=1)\n",
    "        logp = tf.reduce_sum(tf.one_hot(a, depth=adim) * logp_all, axis=1)\n",
    "        logp_pi = tf.reduce_sum(tf.one_hot(pi, depth=adim) * logp_all, axis=1)\n",
    "        return pi, logp, logp_pi, pi\n",
    "    \n",
    "    def gaussian_likelihood(x, mu, log_std):\n",
    "        EPS = 1e-8\n",
    "        pre_sum = -0.5 * (((x-mu)/(tf.exp(log_std)+EPS))**2 + 2*log_std + np.log(2*np.pi))\n",
    "        return tf.reduce_sum(pre_sum, axis=1)\n",
    "    \n",
    "    def mlp_gaussian_policy(o, a, hdims=[64,64], actv=tf.nn.relu, output_actv=None, action_space=None):\n",
    "        adim = a.shape.as_list()[-1]\n",
    "        mu = mlp(x=o, hdims=hdims+[adim], actv=actv, output_actv=output_actv)\n",
    "        log_std = tf.get_variable(name='log_std', initializer=-0.5*np.ones(adim, dtype=np.float32))\n",
    "        std = tf.exp(log_std)\n",
    "        pi = mu + tf.random_normal(tf.shape(mu)) * std\n",
    "        logp = gaussian_likelihood(a, mu, log_std)\n",
    "        logp_pi = gaussian_likelihood(pi, mu, log_std)\n",
    "        return pi, logp, logp_pi, mu # <= mu is added for the deterministic policy\n",
    "    \n",
    "    def mlp_actor_critic(o, a, hdims=[64,64], actv=tf.nn.relu, \n",
    "                     output_actv=None, policy=None, action_space=None):\n",
    "        if policy is None and isinstance(action_space, Box):\n",
    "            policy = mlp_gaussian_policy\n",
    "        elif policy is None and isinstance(action_space, Discrete):\n",
    "            policy = mlp_categorical_policy\n",
    "\n",
    "        with tf.variable_scope('pi'):\n",
    "            pi, logp, logp_pi, mu = policy(\n",
    "                o=o, a=a, hdims=hdims, actv=actv, output_actv=output_actv, action_space=action_space)\n",
    "        with tf.variable_scope('v'):\n",
    "            v = tf.squeeze(mlp(x=o, hdims=hdims+[1], actv=actv, output_actv=None), axis=1)\n",
    "        return pi, logp, logp_pi, v, mu\n",
    "    \n",
    "    def placeholder(dim=None):\n",
    "        return tf.placeholder(dtype=tf.float32,shape=(None,dim) if dim else (None,))\n",
    "    \n",
    "    def placeholders(*args):\n",
    "        \"\"\"\n",
    "        Usage: a_ph,b_ph,c_ph = placeholders(adim,bdim,None)\n",
    "        \"\"\"\n",
    "        return [placeholder(dim) for dim in args]\n",
    "    \n",
    "    # Have own session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    \n",
    "    # Placeholders\n",
    "    odim = env.observation_space.shape[0]\n",
    "    adim = env.action_space.shape[0]\n",
    "    o_ph,a_ph,adv_ph,ret_ph,logp_old_ph = placeholders(odim,adim,None,None,None)\n",
    "    \n",
    "    # Actor-critic model \n",
    "    ac_kwargs = dict()\n",
    "    ac_kwargs['action_space'] = env.action_space\n",
    "    actor_critic = mlp_actor_critic\n",
    "    pi,logp,logp_pi,v,mu = actor_critic(o_ph, a_ph, **ac_kwargs)\n",
    "    \n",
    "    # Need all placeholders in *this* order later (to zip with data from buffer)\n",
    "    all_phs = [o_ph, a_ph, adv_ph, ret_ph, logp_old_ph]\n",
    "    \n",
    "    # Every step, get: action, value, and logprob\n",
    "    get_action_ops = [pi, v, logp_pi]\n",
    "    \n",
    "    # Accumulate model\n",
    "    model = {'o_ph':o_ph,'a_ph':a_ph,'adv_ph':adv_ph,'ret_ph':ret_ph,'logp_old_ph':logp_old_ph,\n",
    "             'pi':pi,'logp':logp,'logp_pi':logp_pi,'v':v,'mu':mu,\n",
    "             'all_phs':all_phs,'get_action_ops':get_action_ops}\n",
    "    return model,sess\n",
    "\n",
    "def create_ppo_graph(model,clip_ratio=0.2,pi_lr=3e-4,vf_lr=1e-3):\n",
    "    \"\"\"\n",
    "    Create PPO Graph\n",
    "    \"\"\"\n",
    "    # PPO objectives\n",
    "    ratio = tf.exp(model['logp'] - model['logp_old_ph']) # pi(a|s) / pi_old(a|s)\n",
    "    min_adv = tf.where(model['adv_ph']>0,\n",
    "                       (1+clip_ratio)*model['adv_ph'], (1-clip_ratio)*model['adv_ph'])\n",
    "    pi_loss = -tf.reduce_mean(tf.minimum(ratio * model['adv_ph'], min_adv))\n",
    "    v_loss = tf.reduce_mean((model['ret_ph'] - model['v'])**2)\n",
    "    \n",
    "    # Info (useful to watch during learning)\n",
    "    approx_kl = tf.reduce_mean(model['logp_old_ph'] - model['logp']) # a sample estimate for KL-divergence\n",
    "    approx_ent = tf.reduce_mean(-model['logp']) # a sample estimate for entropy\n",
    "    clipped = tf.logical_or(ratio > (1+clip_ratio), ratio < (1-clip_ratio))\n",
    "    clipfrac = tf.reduce_mean(tf.cast(clipped, tf.float32))\n",
    "    \n",
    "    # Optimizers\n",
    "    train_pi = tf.train.AdamOptimizer(learning_rate=pi_lr).minimize(pi_loss)\n",
    "    train_v = tf.train.AdamOptimizer(learning_rate=vf_lr).minimize(v_loss)\n",
    "    \n",
    "    # Accumulate graph\n",
    "    graph = {'pi_loss':pi_loss,'v_loss':v_loss,'approx_kl':approx_kl,'approx_ent':approx_ent,\n",
    "             'clipfrac':clipfrac,'train_pi':train_pi,'train_v':train_v}\n",
    "    return graph\n",
    "\n",
    "def update_ppo(model,graph,sess,buf,train_pi_iters=100,train_v_iters=100,target_kl=0.01):\n",
    "    \"\"\"\n",
    "    Update PPO\n",
    "    \"\"\"\n",
    "    feeds = {k:v for k,v in zip(model['all_phs'], buf.get())}\n",
    "    pi_l_old, v_l_old, ent = sess.run(\n",
    "        [graph['pi_loss'],graph['v_loss'],graph['approx_ent']],feed_dict=feeds)\n",
    "    # Training\n",
    "    for i in range(train_pi_iters):\n",
    "        _, kl = sess.run([graph['train_pi'],graph['approx_kl']],feed_dict=feeds)\n",
    "        if kl > 1.5 * target_kl:\n",
    "            break\n",
    "    for _ in range(train_v_iters):\n",
    "        sess.run(graph['train_v'],feed_dict=feeds)\n",
    "    # Log changes from update\n",
    "    pi_l_new,v_l_new,kl,cf = sess.run(\n",
    "        [graph['pi_loss'],graph['v_loss'],graph['approx_kl'],graph['clipfrac']],\n",
    "        feed_dict=feeds)\n",
    "\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_RENDER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AntBulletEnv-v0] ready.\n",
      "odim:[28] adim:[8].\n"
     ]
    }
   ],
   "source": [
    "gym.logger.set_level(40)\n",
    "env_name = 'AntBulletEnv-v0'\n",
    "env,test_env = gym.make(env_name),gym.make(env_name)\n",
    "if DO_RENDER:\n",
    "    _ = test_env.render(mode='human') # enable rendering on test_env\n",
    "_ = test_env.reset()\n",
    "for _ in range(3): # dummy run for proper rendering \n",
    "    a = test_env.action_space.sample()\n",
    "    o,r,d,_ = test_env.step(a)\n",
    "    time.sleep(0.01)\n",
    "print (\"[%s] ready.\"%(env_name))\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space # -1.0 ~ +1.0\n",
    "odim,adim = observation_space.shape[0],action_space.shape[0]\n",
    "print (\"odim:[%d] adim:[%d].\"%(odim,adim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "hdims = [256,256]\n",
    "# Graph\n",
    "clip_ratio = 0.2\n",
    "pi_lr = 3e-4\n",
    "vf_lr = 1e-3\n",
    "# Buffer\n",
    "steps_per_epoch = 5000\n",
    "gamma = 0.99\n",
    "lam = 0.95\n",
    "# Update\n",
    "train_pi_iters = 100\n",
    "train_v_iters = 100\n",
    "target_kl = 0.01\n",
    "epochs = 1000\n",
    "max_ep_len = 1000\n",
    "evaluate_every,num_eval = 10,3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "model,sess = create_ppo_model(env=env,hdims=hdims)\n",
    "graph = create_ppo_graph(model,clip_ratio=clip_ratio,pi_lr=pi_lr,vf_lr=vf_lr)\n",
    "buf = PPOBuffer(odim=odim,adim=adim,size=steps_per_epoch,gamma=gamma,lam=lam)\n",
    "print (\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:[1/1000][0.0%] #step:[5.0e+03] time:[00:00:07] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[9.4461] ep_len:[22]\n",
      " [Evaluate] [1/3] ep_ret:[10.1665] ep_len:[23]\n",
      " [Evaluate] [2/3] ep_ret:[7.0727] ep_len:[21]\n",
      "step:[10/1000][0.9%] #step:[5.0e+04] time:[00:01:15] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[211.3721] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[217.4033] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[214.8042] ep_len:[1000]\n",
      "step:[20/1000][1.9%] #step:[1.0e+05] time:[00:02:33] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[427.1941] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[404.5370] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[469.0417] ep_len:[1000]\n",
      "step:[30/1000][2.9%] #step:[1.5e+05] time:[00:03:51] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[586.0384] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[581.9152] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[304.6696] ep_len:[1000]\n",
      "step:[40/1000][3.9%] #step:[2.0e+05] time:[00:05:08] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[692.4815] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[572.9672] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[570.2875] ep_len:[1000]\n",
      "step:[50/1000][4.9%] #step:[2.5e+05] time:[00:06:24] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[695.8460] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[702.7898] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[593.2900] ep_len:[1000]\n",
      "step:[60/1000][5.9%] #step:[3.0e+05] time:[00:07:41] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[719.3810] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[590.2277] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[433.7886] ep_len:[1000]\n",
      "step:[70/1000][6.9%] #step:[3.5e+05] time:[00:08:57] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[497.8486] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[578.9172] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[603.4435] ep_len:[1000]\n",
      "step:[80/1000][7.9%] #step:[4.0e+05] time:[00:10:14] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[507.5062] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[643.4068] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[794.6309] ep_len:[1000]\n",
      "step:[90/1000][8.9%] #step:[4.5e+05] time:[00:11:30] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[699.9531] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[687.1331] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[723.1339] ep_len:[1000]\n",
      "step:[100/1000][9.9%] #step:[5.0e+05] time:[00:12:46] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[589.2088] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[748.9688] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[778.8092] ep_len:[1000]\n",
      "step:[110/1000][10.9%] #step:[5.5e+05] time:[00:14:03] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[799.8515] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[563.8039] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[605.5674] ep_len:[1000]\n",
      "step:[120/1000][11.9%] #step:[6.0e+05] time:[00:15:19] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[701.9851] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[532.5517] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[594.3281] ep_len:[1000]\n",
      "step:[130/1000][12.9%] #step:[6.5e+05] time:[00:16:34] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[315.7791] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[778.2033] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[738.6294] ep_len:[1000]\n",
      "step:[140/1000][13.9%] #step:[7.0e+05] time:[00:17:51] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[720.2218] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[608.7183] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[810.0212] ep_len:[1000]\n",
      "step:[150/1000][14.9%] #step:[7.5e+05] time:[00:19:07] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[603.3462] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[770.4043] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[865.6628] ep_len:[1000]\n",
      "step:[160/1000][15.9%] #step:[8.0e+05] time:[00:20:23] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[738.3607] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[714.3127] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[811.8446] ep_len:[1000]\n",
      "step:[170/1000][16.9%] #step:[8.5e+05] time:[00:21:39] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[575.2588] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[774.6402] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[723.6360] ep_len:[1000]\n",
      "step:[180/1000][17.9%] #step:[9.0e+05] time:[00:22:56] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[738.4843] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[898.4743] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[900.6678] ep_len:[1000]\n",
      "step:[190/1000][18.9%] #step:[9.5e+05] time:[00:24:12] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[800.1621] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[897.5628] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[752.5488] ep_len:[1000]\n",
      "step:[200/1000][19.9%] #step:[1.0e+06] time:[00:25:28] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[846.7117] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[687.6932] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[827.0577] ep_len:[1000]\n",
      "step:[210/1000][20.9%] #step:[1.0e+06] time:[00:26:44] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[862.4230] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[901.6724] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[891.5049] ep_len:[1000]\n",
      "step:[220/1000][21.9%] #step:[1.1e+06] time:[00:28:00] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[680.7743] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[824.7875] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[685.5919] ep_len:[1000]\n",
      "step:[230/1000][22.9%] #step:[1.2e+06] time:[00:29:16] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[901.8721] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[772.1796] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[890.5728] ep_len:[1000]\n",
      "step:[240/1000][23.9%] #step:[1.2e+06] time:[00:30:32] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[943.7432] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[916.5592] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[806.2629] ep_len:[1000]\n",
      "step:[250/1000][24.9%] #step:[1.2e+06] time:[00:31:48] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[936.9985] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[819.2199] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[905.0866] ep_len:[1000]\n",
      "step:[260/1000][25.9%] #step:[1.3e+06] time:[00:33:05] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[799.0700] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[798.2087] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[823.6896] ep_len:[1000]\n",
      "step:[270/1000][26.9%] #step:[1.4e+06] time:[00:34:21] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1015.0984] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[836.9245] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1022.0393] ep_len:[1000]\n",
      "step:[280/1000][27.9%] #step:[1.4e+06] time:[00:35:37] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[856.1893] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[670.2461] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[939.0311] ep_len:[1000]\n",
      "step:[290/1000][28.9%] #step:[1.4e+06] time:[00:36:53] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[879.7747] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[853.7585] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1011.2386] ep_len:[1000]\n",
      "step:[300/1000][29.9%] #step:[1.5e+06] time:[00:38:09] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[867.1185] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[727.1001] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1044.7658] ep_len:[1000]\n",
      "step:[310/1000][30.9%] #step:[1.6e+06] time:[00:39:25] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[997.1673] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1105.6349] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[690.0868] ep_len:[1000]\n",
      "step:[320/1000][31.9%] #step:[1.6e+06] time:[00:40:41] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1128.2890] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1136.1008] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1120.3677] ep_len:[1000]\n",
      "step:[330/1000][32.9%] #step:[1.6e+06] time:[00:41:57] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[843.4649] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1174.5546] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[782.2714] ep_len:[1000]\n",
      "step:[340/1000][33.9%] #step:[1.7e+06] time:[00:43:13] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1219.4108] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1070.8628] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1111.3862] ep_len:[1000]\n",
      "step:[350/1000][34.9%] #step:[1.8e+06] time:[00:44:28] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1188.0919] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1158.7884] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[980.2955] ep_len:[1000]\n",
      "step:[360/1000][35.9%] #step:[1.8e+06] time:[00:45:44] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1001.9272] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1205.6533] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1097.8843] ep_len:[1000]\n",
      "step:[370/1000][36.9%] #step:[1.8e+06] time:[00:47:00] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1158.1914] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[740.7260] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1198.1088] ep_len:[1000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:[380/1000][37.9%] #step:[1.9e+06] time:[00:48:16] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1132.8869] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1266.8313] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1000.8666] ep_len:[1000]\n",
      "step:[390/1000][38.9%] #step:[2.0e+06] time:[00:49:32] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1261.7295] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1375.3519] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1301.5249] ep_len:[1000]\n",
      "step:[400/1000][39.9%] #step:[2.0e+06] time:[00:50:47] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1384.8669] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1068.9638] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1188.6553] ep_len:[1000]\n",
      "step:[410/1000][40.9%] #step:[2.0e+06] time:[00:52:03] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1307.8832] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1218.8203] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1390.3460] ep_len:[1000]\n",
      "step:[420/1000][41.9%] #step:[2.1e+06] time:[00:53:19] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1316.8750] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1350.0388] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1293.1982] ep_len:[1000]\n",
      "step:[430/1000][42.9%] #step:[2.2e+06] time:[00:54:34] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1461.5025] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1522.9432] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1406.5737] ep_len:[1000]\n",
      "step:[440/1000][43.9%] #step:[2.2e+06] time:[00:55:49] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1454.4478] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1473.6306] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1452.5908] ep_len:[1000]\n",
      "step:[450/1000][44.9%] #step:[2.2e+06] time:[00:57:05] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1616.8509] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1415.1862] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1544.8185] ep_len:[1000]\n",
      "step:[460/1000][45.9%] #step:[2.3e+06] time:[00:58:20] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1656.4387] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1625.3758] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1438.7431] ep_len:[1000]\n",
      "step:[470/1000][46.9%] #step:[2.4e+06] time:[00:59:36] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1526.5150] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1562.3642] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1647.0262] ep_len:[1000]\n",
      "step:[480/1000][47.9%] #step:[2.4e+06] time:[01:00:51] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1564.2345] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1546.6362] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1635.6812] ep_len:[1000]\n",
      "step:[490/1000][48.9%] #step:[2.4e+06] time:[01:02:06] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1585.3027] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1572.2049] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1535.8030] ep_len:[1000]\n",
      "step:[500/1000][49.9%] #step:[2.5e+06] time:[01:03:22] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1735.9865] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1712.3860] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1733.1149] ep_len:[1000]\n",
      "step:[510/1000][50.9%] #step:[2.6e+06] time:[01:04:37] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1591.8043] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1647.6473] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1605.5167] ep_len:[1000]\n",
      "step:[520/1000][51.9%] #step:[2.6e+06] time:[01:05:53] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1749.0913] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1683.6719] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1709.0817] ep_len:[1000]\n",
      "step:[530/1000][52.9%] #step:[2.6e+06] time:[01:07:08] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1582.9906] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1744.6512] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1758.6789] ep_len:[1000]\n",
      "step:[540/1000][53.9%] #step:[2.7e+06] time:[01:08:23] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1750.2987] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1761.2550] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1732.5399] ep_len:[1000]\n",
      "step:[550/1000][54.9%] #step:[2.8e+06] time:[01:09:39] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1810.1514] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1806.1106] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1848.4787] ep_len:[1000]\n",
      "step:[560/1000][55.9%] #step:[2.8e+06] time:[01:10:54] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1863.7427] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1849.7736] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1853.0621] ep_len:[1000]\n",
      "step:[570/1000][56.9%] #step:[2.8e+06] time:[01:12:10] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1836.2232] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1865.9309] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1864.0360] ep_len:[1000]\n",
      "step:[580/1000][57.9%] #step:[2.9e+06] time:[01:13:25] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1908.5622] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1897.5211] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1901.2487] ep_len:[1000]\n",
      "step:[590/1000][58.9%] #step:[3.0e+06] time:[01:14:41] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1907.2736] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1918.9126] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1917.8753] ep_len:[1000]\n",
      "step:[600/1000][59.9%] #step:[3.0e+06] time:[01:15:57] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1892.3149] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1877.0711] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1874.7414] ep_len:[1000]\n",
      "step:[610/1000][60.9%] #step:[3.0e+06] time:[01:17:12] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1911.0839] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1910.2846] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1910.6346] ep_len:[1000]\n",
      "step:[620/1000][61.9%] #step:[3.1e+06] time:[01:18:28] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1923.8344] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1948.7149] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1965.4770] ep_len:[1000]\n",
      "step:[630/1000][62.9%] #step:[3.2e+06] time:[01:19:43] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1964.1491] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1966.8023] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1952.2299] ep_len:[1000]\n",
      "step:[640/1000][63.9%] #step:[3.2e+06] time:[01:20:58] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1844.1185] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1835.7166] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1851.8082] ep_len:[1000]\n",
      "step:[650/1000][64.9%] #step:[3.2e+06] time:[01:22:13] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1849.4522] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1891.2507] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1904.4082] ep_len:[1000]\n",
      "step:[660/1000][65.9%] #step:[3.3e+06] time:[01:23:28] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1968.4463] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1938.8535] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1942.8654] ep_len:[1000]\n",
      "step:[670/1000][66.9%] #step:[3.4e+06] time:[01:24:43] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2016.5854] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1979.4183] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2012.8424] ep_len:[1000]\n",
      "step:[680/1000][67.9%] #step:[3.4e+06] time:[01:25:59] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2022.8314] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2065.0047] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2037.3737] ep_len:[1000]\n",
      "step:[690/1000][68.9%] #step:[3.4e+06] time:[01:27:14] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1960.6170] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1989.1338] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1813.5032] ep_len:[1000]\n",
      "step:[700/1000][69.9%] #step:[3.5e+06] time:[01:28:29] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2033.3772] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1970.1193] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1987.7090] ep_len:[1000]\n",
      "step:[710/1000][70.9%] #step:[3.6e+06] time:[01:29:44] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2011.3199] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1929.5899] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2030.8277] ep_len:[1000]\n",
      "step:[720/1000][71.9%] #step:[3.6e+06] time:[01:31:00] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1972.5596] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1992.1801] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1999.9917] ep_len:[1000]\n",
      "step:[730/1000][72.9%] #step:[3.6e+06] time:[01:32:15] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1974.7957] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2056.8948] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2017.8585] ep_len:[1000]\n",
      "step:[740/1000][73.9%] #step:[3.7e+06] time:[01:33:30] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2064.7083] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1942.1764] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1990.8270] ep_len:[1000]\n",
      "step:[750/1000][74.9%] #step:[3.8e+06] time:[01:34:45] ram:[3.8%].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Evaluate] [0/3] ep_ret:[2043.7888] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2070.8263] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2070.5895] ep_len:[1000]\n",
      "step:[760/1000][75.9%] #step:[3.8e+06] time:[01:36:01] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2068.8236] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2082.9449] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2057.1212] ep_len:[1000]\n",
      "step:[770/1000][76.9%] #step:[3.8e+06] time:[01:37:16] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2144.3987] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2094.5138] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2156.7927] ep_len:[1000]\n",
      "step:[780/1000][77.9%] #step:[3.9e+06] time:[01:38:31] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2021.1463] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2073.7606] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1977.5640] ep_len:[1000]\n",
      "step:[790/1000][78.9%] #step:[4.0e+06] time:[01:39:46] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2015.7091] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2043.6343] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2079.3064] ep_len:[1000]\n",
      "step:[800/1000][79.9%] #step:[4.0e+06] time:[01:41:01] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2119.1945] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2205.3129] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2151.5159] ep_len:[1000]\n",
      "step:[810/1000][80.9%] #step:[4.0e+06] time:[01:42:16] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2127.7836] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2133.8792] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2071.4805] ep_len:[1000]\n",
      "step:[820/1000][81.9%] #step:[4.1e+06] time:[01:43:31] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2205.6581] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2140.7448] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2117.1778] ep_len:[1000]\n",
      "step:[830/1000][82.9%] #step:[4.2e+06] time:[01:44:47] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2174.4021] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2006.0514] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2136.3623] ep_len:[1000]\n",
      "step:[840/1000][83.9%] #step:[4.2e+06] time:[01:46:02] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1965.2335] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2042.9002] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2097.2886] ep_len:[1000]\n",
      "step:[850/1000][84.9%] #step:[4.2e+06] time:[01:47:17] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2057.6207] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2060.4626] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2051.6881] ep_len:[1000]\n",
      "step:[860/1000][85.9%] #step:[4.3e+06] time:[01:48:32] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1934.7421] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1969.5587] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2129.0855] ep_len:[1000]\n",
      "step:[870/1000][86.9%] #step:[4.4e+06] time:[01:49:47] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2004.5971] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2021.7103] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1997.6581] ep_len:[1000]\n",
      "step:[880/1000][87.9%] #step:[4.4e+06] time:[01:51:02] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[1126.8916] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2104.9524] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2078.7486] ep_len:[1000]\n",
      "step:[890/1000][88.9%] #step:[4.4e+06] time:[01:52:17] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2094.8456] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2065.6861] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1959.7970] ep_len:[1000]\n",
      "step:[900/1000][89.9%] #step:[4.5e+06] time:[01:53:33] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2075.0868] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2052.0974] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2019.1281] ep_len:[1000]\n",
      "step:[910/1000][90.9%] #step:[4.6e+06] time:[01:54:48] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2124.0993] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2140.9203] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2137.6779] ep_len:[1000]\n",
      "step:[920/1000][91.9%] #step:[4.6e+06] time:[01:56:03] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2116.6117] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2117.2468] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2096.0936] ep_len:[1000]\n",
      "step:[930/1000][92.9%] #step:[4.6e+06] time:[01:57:18] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2108.1268] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2100.0330] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2083.8657] ep_len:[1000]\n",
      "step:[940/1000][93.9%] #step:[4.7e+06] time:[01:58:34] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2157.5394] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2164.0761] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2151.7899] ep_len:[1000]\n",
      "step:[950/1000][94.9%] #step:[4.8e+06] time:[01:59:49] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2142.6295] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2125.9789] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2212.8555] ep_len:[1000]\n",
      "step:[960/1000][95.9%] #step:[4.8e+06] time:[02:01:04] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2207.5806] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2210.0245] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2202.8217] ep_len:[1000]\n",
      "step:[970/1000][96.9%] #step:[4.8e+06] time:[02:02:19] ram:[3.8%].\n",
      " [Evaluate] [0/3] ep_ret:[2119.9052] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2140.5634] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2124.3155] ep_len:[1000]\n",
      "step:[980/1000][97.9%] #step:[4.9e+06] time:[02:03:34] ram:[3.9%].\n",
      " [Evaluate] [0/3] ep_ret:[2092.4292] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2091.3927] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2080.5179] ep_len:[1000]\n",
      "step:[990/1000][98.9%] #step:[5.0e+06] time:[02:04:49] ram:[3.9%].\n",
      " [Evaluate] [0/3] ep_ret:[2185.1027] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2152.9086] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2171.1133] ep_len:[1000]\n",
      "step:[1000/1000][99.9%] #step:[5.0e+06] time:[02:06:05] ram:[3.9%].\n",
      " [Evaluate] [0/3] ep_ret:[2188.8775] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2112.6193] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2210.7055] ep_len:[1000]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "start_time = time.time()\n",
    "o,r,d,ep_ret,ep_len,n_env_step = env.reset(),0,False,0,0,0\n",
    "# Main loop: collect experience in env and update/log each epoch\n",
    "for epoch in range(epochs):\n",
    "    for t in range(steps_per_epoch):\n",
    "        a,v_t,logp_t = sess.run(\n",
    "            model['get_action_ops'],feed_dict={model['o_ph']:o.reshape(1,-1)})\n",
    "\n",
    "        o2, r, d, _ = env.step(a[0])\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "        n_env_step += 1\n",
    "\n",
    "        # save and log\n",
    "        buf.store(o, a, r, v_t, logp_t)\n",
    "\n",
    "        # Update obs (critical!)\n",
    "        o = o2\n",
    "\n",
    "        terminal = d or (ep_len == max_ep_len)\n",
    "        if terminal or (t==steps_per_epoch-1):\n",
    "            last_val = 0 if d else sess.run(\n",
    "                model['v'],feed_dict={model['o_ph']: o.reshape(1,-1)})\n",
    "            buf.finish_path(last_val)\n",
    "            o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Perform PPO update!\n",
    "    update_ppo(model=model,graph=graph,sess=sess,buf=buf,\n",
    "               train_pi_iters=train_pi_iters,train_v_iters=train_v_iters,\n",
    "               target_kl=target_kl)\n",
    "    \n",
    "    # Evaluate\n",
    "    if (epoch==0) or (((epoch+1)%evaluate_every) == 0):\n",
    "        ram_percent = psutil.virtual_memory().percent # memory usage\n",
    "        print (\"step:[%d/%d][%.1f%%] #step:[%.1e] time:[%s] ram:[%.1f%%].\"%\n",
    "               (epoch+1,epochs,epoch/epochs*100,\n",
    "                n_env_step,\n",
    "                time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time)),\n",
    "                ram_percent)\n",
    "              )\n",
    "        for eval_idx in range(num_eval): \n",
    "            o,d,ep_ret,ep_len = test_env.reset(),False,0,0\n",
    "            if DO_RENDER:\n",
    "                _ = test_env.render(mode='human') \n",
    "            while not(d or (ep_len == max_ep_len)):\n",
    "                a = sess.run(model['mu'],feed_dict={model['o_ph']:o.reshape(1,-1)})\n",
    "                o,r,d,_ = test_env.step(a[0])\n",
    "                if DO_RENDER:\n",
    "                    _ = test_env.render(mode='human') \n",
    "                ep_ret += r # compute return \n",
    "                ep_len += 1\n",
    "            print (\" [Evaluate] [%d/%d] ep_ret:[%.4f] ep_len:[%d]\"%\n",
    "                   (eval_idx,num_eval,ep_ret,ep_len))\n",
    "\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env closed.\n"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "test_env.close()\n",
    "print (\"Env closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluate] ep_ret:[2218.5393] ep_len:[1000]\n"
     ]
    }
   ],
   "source": [
    "gym.logger.set_level(40)\n",
    "env_name = 'AntBulletEnv-v0'\n",
    "test_env = gym.make(env_name)\n",
    "o,d,ep_ret,ep_len = test_env.reset(),False,0,0\n",
    "if DO_RENDER:\n",
    "    _ = test_env.render(mode='human') \n",
    "while not(d or (ep_len == max_ep_len)):\n",
    "    a = sess.run(model['mu'],feed_dict={model['o_ph']:o.reshape(1,-1)})\n",
    "    o,r,d,_ = test_env.step(a[0])\n",
    "    if DO_RENDER:\n",
    "        _ = test_env.render(mode='human') \n",
    "    ep_ret += r # compute return \n",
    "    ep_len += 1\n",
    "print (\"[Evaluate] ep_ret:[%.4f] ep_len:[%d]\"\n",
    "    %(ep_ret,ep_len))\n",
    "test_env.close() # close env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
