{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchronous PPO with PyBullet Ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "import datetime,gym,os,pybullet_envs,time,os,psutil,ray\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from util import gpu_sess,suppress_tf_warning,suppress_gym_warning\n",
    "np.set_printoptions(precision=2)\n",
    "suppress_tf_warning() \n",
    "suppress_gym_warning()\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO helper functions (most of them are copied from spinning up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "import scipy.signal\n",
    "\n",
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def statistics_scalar(x, with_min_and_max=False):\n",
    "    \"\"\"\n",
    "    Get mean/std and optional min/max of scalar x \n",
    "    Args:\n",
    "        x: An array containing samples of the scalar to produce statistics for.\n",
    "        with_min_and_max (bool): If true, return min and max of x in \n",
    "            addition to mean and std.\n",
    "    \"\"\"\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    global_sum, global_n = np.sum(x), len(x)\n",
    "    mean = global_sum / global_n\n",
    "    global_sum_sq = np.sum((x - mean)**2)\n",
    "    std = np.sqrt(global_sum_sq / global_n)  # compute global std\n",
    "    if with_min_and_max:\n",
    "        global_min = (np.min(x) if len(x) > 0 else np.inf)\n",
    "        global_max = (np.max(x) if len(x) > 0 else -np.inf)\n",
    "        return mean, std, global_min, global_max\n",
    "    return mean, std\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    Compute discounted cumulative sums of vectors.\n",
    "    input: \n",
    "        vector x, [x0, x1, x2]\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  \n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "class PPOBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a PPO agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "    def __init__(self, odim, adim, size=5000, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, odim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, adim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = statistics_scalar(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        return [self.obs_buf, self.act_buf, self.adv_buf, \n",
    "                self.ret_buf, self.logp_buf]\n",
    "        \n",
    "def create_ppo_model(env=None,hdims=[256,256],output_actv=None):\n",
    "    \"\"\"\n",
    "    Create PPO Actor-Critic Model (compatible with Ray)\n",
    "    \"\"\"\n",
    "    import tensorflow as tf # make it compatible with Ray actors\n",
    "    from gym.spaces import Box, Discrete\n",
    "    \n",
    "    def mlp(x, hdims=[64,64], actv=tf.nn.relu, output_actv=None):\n",
    "        for h in hdims[:-1]:\n",
    "            x = tf.layers.dense(x, units=h, activation=actv)\n",
    "        return tf.layers.dense(x, units=hdims[-1], activation=output_actv)\n",
    "    \n",
    "    def mlp_categorical_policy(o, a, hdims=[64,64], actv=tf.nn.relu, output_actv=None, action_space=None):\n",
    "        adim = action_space.n\n",
    "        logits = mlp(x=o, hdims=hdims+[adim], actv=actv, output_actv=None)\n",
    "        logp_all = tf.nn.log_softmax(logits)\n",
    "        pi = tf.squeeze(tf.multinomial(logits,1), axis=1)\n",
    "        logp = tf.reduce_sum(tf.one_hot(a, depth=adim) * logp_all, axis=1)\n",
    "        logp_pi = tf.reduce_sum(tf.one_hot(pi, depth=adim) * logp_all, axis=1)\n",
    "        return pi, logp, logp_pi, pi\n",
    "    \n",
    "    def gaussian_likelihood(x, mu, log_std):\n",
    "        EPS = 1e-8\n",
    "        pre_sum = -0.5 * (((x-mu)/(tf.exp(log_std)+EPS))**2 + 2*log_std + np.log(2*np.pi))\n",
    "        return tf.reduce_sum(pre_sum, axis=1)\n",
    "    \n",
    "    def mlp_gaussian_policy(o, a, hdims=[64,64], actv=tf.nn.relu, output_actv=None, action_space=None):\n",
    "        adim = a.shape.as_list()[-1]\n",
    "        mu = mlp(x=o, hdims=hdims+[adim], actv=actv, output_actv=output_actv)\n",
    "        log_std = tf.get_variable(name='log_std', initializer=-0.5*np.ones(adim, dtype=np.float32))\n",
    "        std = tf.exp(log_std)\n",
    "        pi = mu + tf.random_normal(tf.shape(mu)) * std\n",
    "        logp = gaussian_likelihood(a, mu, log_std)\n",
    "        logp_pi = gaussian_likelihood(pi, mu, log_std)\n",
    "        return pi, logp, logp_pi, mu # <= mu is added for the deterministic policy\n",
    "    \n",
    "    def mlp_actor_critic(o, a, hdims=[64,64], actv=tf.nn.relu, \n",
    "                     output_actv=None, policy=None, action_space=None):\n",
    "        if policy is None and isinstance(action_space, Box):\n",
    "            policy = mlp_gaussian_policy\n",
    "        elif policy is None and isinstance(action_space, Discrete):\n",
    "            policy = mlp_categorical_policy\n",
    "\n",
    "        with tf.variable_scope('pi'):\n",
    "            pi, logp, logp_pi, mu = policy(\n",
    "                o=o, a=a, hdims=hdims, actv=actv, output_actv=output_actv, action_space=action_space)\n",
    "        with tf.variable_scope('v'):\n",
    "            v = tf.squeeze(mlp(x=o, hdims=hdims+[1], actv=actv, output_actv=None), axis=1)\n",
    "        return pi, logp, logp_pi, v, mu\n",
    "    \n",
    "    def placeholder(dim=None):\n",
    "        return tf.placeholder(dtype=tf.float32,shape=(None,dim) if dim else (None,))\n",
    "    \n",
    "    def placeholders(*args):\n",
    "        \"\"\"\n",
    "        Usage: a_ph,b_ph,c_ph = placeholders(adim,bdim,None)\n",
    "        \"\"\"\n",
    "        return [placeholder(dim) for dim in args]\n",
    "    \n",
    "    def get_vars(scope):\n",
    "        return [x for x in tf.compat.v1.global_variables() if scope in x.name]\n",
    "    \n",
    "    # Have own session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    \n",
    "    # Placeholders\n",
    "    odim = env.observation_space.shape[0]\n",
    "    adim = env.action_space.shape[0]\n",
    "    o_ph,a_ph,adv_ph,ret_ph,logp_old_ph = placeholders(odim,adim,None,None,None)\n",
    "    \n",
    "    # Actor-critic model \n",
    "    ac_kwargs = dict()\n",
    "    ac_kwargs['action_space'] = env.action_space\n",
    "    actor_critic = mlp_actor_critic\n",
    "    pi,logp,logp_pi,v,mu = actor_critic(o_ph, a_ph, **ac_kwargs)\n",
    "    \n",
    "    # Need all placeholders in *this* order later (to zip with data from buffer)\n",
    "    all_phs = [o_ph, a_ph, adv_ph, ret_ph, logp_old_ph]\n",
    "    \n",
    "    # Every step, get: action, value, and logprob\n",
    "    get_action_ops = [pi, v, logp_pi]\n",
    "    \n",
    "    # Get variables\n",
    "    pi_vars,v_vars = get_vars('pi'),get_vars('v')\n",
    "    \n",
    "    # Accumulate model\n",
    "    model = {'o_ph':o_ph,'a_ph':a_ph,'adv_ph':adv_ph,'ret_ph':ret_ph,'logp_old_ph':logp_old_ph,\n",
    "             'pi':pi,'logp':logp,'logp_pi':logp_pi,'v':v,'mu':mu,\n",
    "             'all_phs':all_phs,'get_action_ops':get_action_ops,'pi_vars':pi_vars,'v_vars':v_vars}\n",
    "    return model,sess\n",
    "\n",
    "def create_ppo_graph(model,clip_ratio=0.2,pi_lr=3e-4,vf_lr=1e-3,epsilon=1e-2):\n",
    "    \"\"\"\n",
    "    Create PPO Graph\n",
    "    \"\"\"\n",
    "    # PPO objectives\n",
    "    ratio = tf.exp(model['logp'] - model['logp_old_ph']) # pi(a|s) / pi_old(a|s)\n",
    "    min_adv = tf.where(model['adv_ph']>0,\n",
    "                       (1+clip_ratio)*model['adv_ph'], (1-clip_ratio)*model['adv_ph'])\n",
    "    pi_loss = -tf.reduce_mean(tf.minimum(ratio * model['adv_ph'], min_adv))\n",
    "    v_loss = tf.reduce_mean((model['ret_ph'] - model['v'])**2)\n",
    "    \n",
    "    # Info (useful to watch during learning)\n",
    "    approx_kl = tf.reduce_mean(model['logp_old_ph'] - model['logp']) # a sample estimate for KL-divergence\n",
    "    approx_ent = tf.reduce_mean(-model['logp']) # a sample estimate for entropy\n",
    "    clipped = tf.logical_or(ratio > (1+clip_ratio), ratio < (1-clip_ratio))\n",
    "    clipfrac = tf.reduce_mean(tf.cast(clipped, tf.float32))\n",
    "    \n",
    "    # Optimizers\n",
    "    pi_ent_loss = pi_loss - 0.01 * approx_ent # entropy-reg policy loss \n",
    "    train_pi = tf.train.AdamOptimizer(learning_rate=pi_lr,epsilon=epsilon).minimize(pi_ent_loss)\n",
    "    train_v = tf.train.AdamOptimizer(learning_rate=vf_lr,epsilon=epsilon).minimize(v_loss)\n",
    "    \n",
    "    # Accumulate graph\n",
    "    graph = {'pi_loss':pi_loss,'v_loss':v_loss,'approx_kl':approx_kl,'approx_ent':approx_ent,\n",
    "             'clipfrac':clipfrac,'train_pi':train_pi,'train_v':train_v}\n",
    "    return graph\n",
    "\n",
    "def update_ppo(model,graph,sess,buf,train_pi_iters=100,train_v_iters=100,target_kl=0.01):\n",
    "    \"\"\"\n",
    "    Update PPO\n",
    "    \"\"\"\n",
    "    feeds = {k:v for k,v in zip(model['all_phs'], buf.get())}\n",
    "    pi_l_old, v_l_old, ent = sess.run(\n",
    "        [graph['pi_loss'],graph['v_loss'],graph['approx_ent']],feed_dict=feeds)\n",
    "    # Training\n",
    "    for i in range(train_pi_iters):\n",
    "        _, kl = sess.run([graph['train_pi'],graph['approx_kl']],feed_dict=feeds)\n",
    "        if kl > 1.5 * target_kl: # if kl exceeds threshold\n",
    "            break\n",
    "    for _ in range(train_v_iters):\n",
    "        sess.run(graph['train_v'],feed_dict=feeds)\n",
    "    # Log changes from update\n",
    "    pi_l_new,v_l_new,kl,cf = sess.run(\n",
    "        [graph['pi_loss'],graph['v_loss'],graph['approx_kl'],graph['clipfrac']],\n",
    "        feed_dict=feeds)\n",
    "    \n",
    "def save_ppo_model(npz_path,R,VERBOSE=True):\n",
    "    \"\"\"\n",
    "    Save PPO model weights\n",
    "    \"\"\"\n",
    "    # PPO model\n",
    "    tf_vars = R.model['pi_vars'] + R.model['v_vars']\n",
    "    data2save,var_names,var_vals = dict(),[],[]\n",
    "    for v_idx,tf_var in enumerate(tf_vars):\n",
    "        var_name,var_val = tf_var.name,R.sess.run(tf_var)\n",
    "        var_names.append(var_name)\n",
    "        var_vals.append(var_val)\n",
    "        data2save[var_name] = var_val\n",
    "        if VERBOSE:\n",
    "            print (\"[%02d]  var_name:[%s]  var_shape:%s\"%\n",
    "                (v_idx,var_name,var_val.shape,)) \n",
    "    \n",
    "    # Create folder if not exist\n",
    "    dir_name = os.path.dirname(npz_path)\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "        print (\"[%s] created.\"%(dir_name))\n",
    "    # Save npz\n",
    "    np.savez(npz_path,**data2save)\n",
    "    print (\"[%s] saved.\"%(npz_path))\n",
    "    \n",
    "def restore_ppo_model(npz_path,R,VERBOSE=True):\n",
    "    \"\"\"\n",
    "    Restore PPO model weights\n",
    "    \"\"\"\n",
    "    # Load npz\n",
    "    l = np.load(npz_path)\n",
    "    print (\"[%s] loaded.\"%(npz_path))\n",
    "    \n",
    "    # Get values of PPO model  \n",
    "    tf_vars = R.model['pi_vars'] + R.model['v_vars']\n",
    "    var_vals = []\n",
    "    for tf_var in tf_vars:\n",
    "        var_vals.append(l[tf_var.name])       \n",
    "    # Assign weights of PPO model\n",
    "    R.set_weights(var_vals)\n",
    "    \n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "def get_env(RENDER=False):\n",
    "    import pybullet_envs,gym\n",
    "    gym.logger.set_level(40) # gym logger\n",
    "    eval_env = gym.make('AntBulletEnv-v0')\n",
    "    if RENDER:\n",
    "        _ = eval_env.render(mode='human') # enable rendering\n",
    "        _ = eval_env.reset()\n",
    "        for _ in range(3): # dummy run for proper rendering \n",
    "            a = eval_env.action_space.sample()\n",
    "            o,r,d,_ = eval_env.step(a)\n",
    "            time.sleep(0.01)\n",
    "    return eval_env\n",
    "\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollout worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "class RolloutWorkerClass(object):\n",
    "    \"\"\"\n",
    "    Worker without RAY (for update purposes)\n",
    "    \"\"\"\n",
    "    def __init__(self,seed=1):\n",
    "        self.seed = seed\n",
    "        # Each worker should maintain its own environment\n",
    "        import pybullet_envs,gym\n",
    "        from util import suppress_tf_warning\n",
    "        suppress_tf_warning() # suppress TF warnings\n",
    "        gym.logger.set_level(40) # gym logger \n",
    "        self.env = get_env()\n",
    "        odim,adim = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        self.odim = odim\n",
    "        self.adim = adim\n",
    "        # Initialize PPO\n",
    "        self.model,self.sess = create_ppo_model(env=self.env,hdims=hdims,output_actv=tf.nn.tanh)\n",
    "        self.graph = create_ppo_graph(self.model,\n",
    "                                      clip_ratio=clip_ratio,pi_lr=pi_lr,vf_lr=vf_lr,epsilon=epsilon)\n",
    "        # Initialize model \n",
    "        tf.set_random_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Flag to initialize assign operations for 'set_weights()'\n",
    "        self.FIRST_SET_FLAG = True\n",
    "        \n",
    "    def get_action(self,o,deterministic=False):\n",
    "        act_op = self.model['mu'] if deterministic else self.model['pi']\n",
    "        return self.sess.run(act_op, feed_dict={self.model['o_ph']:o.reshape(1,-1)})[0]\n",
    "    \n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Get weights\n",
    "        \"\"\"\n",
    "        weight_vals = self.sess.run(self.model['pi_vars']+self.model['v_vars'])\n",
    "        return weight_vals\n",
    "    \n",
    "    def set_weights(self,weight_vals):\n",
    "        \"\"\"\n",
    "        Set weights without memory leakage\n",
    "        \"\"\"\n",
    "        if self.FIRST_SET_FLAG:\n",
    "            self.FIRST_SET_FLAG = False\n",
    "            self.assign_placeholders = []\n",
    "            self.assign_ops = []\n",
    "            for w_idx,weight_tf_var in enumerate(self.model['pi_vars']+self.model['v_vars']):\n",
    "                a = weight_tf_var\n",
    "                assign_placeholder = tf.placeholder(a.dtype, shape=a.get_shape())\n",
    "                assign_op = a.assign(assign_placeholder)\n",
    "                self.assign_placeholders.append(assign_placeholder)\n",
    "                self.assign_ops.append(assign_op)\n",
    "        for w_idx,weight_tf_var in enumerate(self.model['pi_vars']+self.model['v_vars']):\n",
    "            self.sess.run(self.assign_ops[w_idx],\n",
    "                          {self.assign_placeholders[w_idx]:weight_vals[w_idx]})    \n",
    "    \n",
    "# Distributed workers    \n",
    "@ray.remote\n",
    "class RayRolloutWorkerClass(object):\n",
    "    \"\"\"\n",
    "    Rollout Worker with RAY\n",
    "    \"\"\"\n",
    "    def __init__(self,worker_id=0,ep_len_rollout=1000):\n",
    "        # Parse\n",
    "        self.worker_id = worker_id\n",
    "        self.ep_len_rollout = ep_len_rollout\n",
    "        # Each worker should maintain its own environment\n",
    "        import pybullet_envs,gym\n",
    "        from util import suppress_tf_warning\n",
    "        suppress_tf_warning() # suppress TF warnings\n",
    "        gym.logger.set_level(40) # gym logger \n",
    "        self.env = get_env()\n",
    "        odim,adim = self.env.observation_space.shape[0],self.env.action_space.shape[0]\n",
    "        self.odim = odim\n",
    "        self.adim = adim\n",
    "        # Replay buffers to pass\n",
    "        self.o_buffer = np.zeros((self.ep_len_rollout,self.odim))\n",
    "        self.a_buffer = np.zeros((self.ep_len_rollout,self.adim))\n",
    "        self.r_buffer = np.zeros((self.ep_len_rollout))\n",
    "        self.v_t_buffer = np.zeros((self.ep_len_rollout))\n",
    "        self.logp_t_buffer = np.zeros((self.ep_len_rollout))\n",
    "        # Create PPO model\n",
    "        self.model,self.sess = create_ppo_model(env=self.env,hdims=hdims,output_actv=tf.nn.tanh)\n",
    "        # Initialize model \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        # Buffer\n",
    "        self.buf = PPOBuffer(odim=self.odim,adim=self.adim,\n",
    "                             size=ep_len_rollout,gamma=gamma,lam=lam)\n",
    "        \n",
    "        # Flag to initialize assign operations for 'set_weights()'\n",
    "        self.FIRST_SET_FLAG = True\n",
    "        \n",
    "        # Flag to initialize rollout\n",
    "        self.FIRST_ROLLOUT_FLAG = True\n",
    "        \n",
    "        print (\"Worker [%d] initialized.\"%(self.worker_id))\n",
    "        \n",
    "    def get_action(self,o,deterministic=False):\n",
    "        act_op = self.model['mu'] if deterministic else self.model['pi']\n",
    "        return self.sess.run(act_op, feed_dict={self.model['o_ph']:o.reshape(1,-1)})[0]\n",
    "    \n",
    "    def set_weights(self,weight_vals):\n",
    "        \"\"\"\n",
    "        Set weights without memory leakage\n",
    "        \"\"\"\n",
    "        if self.FIRST_SET_FLAG:\n",
    "            self.FIRST_SET_FLAG = False\n",
    "            self.assign_placeholders = []\n",
    "            self.assign_ops = []\n",
    "            for w_idx,weight_tf_var in enumerate(self.model['pi_vars']+self.model['v_vars']):\n",
    "                a = weight_tf_var\n",
    "                assign_placeholder = tf.placeholder(a.dtype, shape=a.get_shape())\n",
    "                assign_op = a.assign(assign_placeholder)\n",
    "                self.assign_placeholders.append(assign_placeholder)\n",
    "                self.assign_ops.append(assign_op)\n",
    "        for w_idx,weight_tf_var in enumerate(self.model['pi_vars']+self.model['v_vars']):\n",
    "            self.sess.run(self.assign_ops[w_idx],\n",
    "                          {self.assign_placeholders[w_idx]:weight_vals[w_idx]})    \n",
    "        \n",
    "    def rollout(self):\n",
    "        \"\"\"\n",
    "        Rollout\n",
    "        \"\"\"\n",
    "        if self.FIRST_ROLLOUT_FLAG:\n",
    "            self.FIRST_ROLLOUT_FLAG = False\n",
    "            self.o = self.env.reset() # reset environment\n",
    "        # Loop\n",
    "        for t in range(ep_len_rollout):\n",
    "            a,v_t,logp_t = self.sess.run(\n",
    "                self.model['get_action_ops'],feed_dict={self.model['o_ph']:self.o.reshape(1,-1)})\n",
    "            o2, r, d, _ = self.env.step(a[0])\n",
    "            # save and log\n",
    "            self.buf.store(self.o,a,r,v_t,logp_t)\n",
    "            # Update obs (critical!)\n",
    "            self.o = o2\n",
    "            if d:\n",
    "                self.buf.finish_path(last_val=0.0)\n",
    "                self.o = self.env.reset() # reset when done \n",
    "        \n",
    "        last_val = self.sess.run(self.model['v'],\n",
    "                                 feed_dict={self.model['o_ph']:self.o.reshape(1,-1)})\n",
    "        self.buf.finish_path(last_val)\n",
    "        return self.buf.get()\n",
    "    \n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "hdims = [256,256]\n",
    "\n",
    "# Graph\n",
    "clip_ratio = 0.2\n",
    "pi_lr = 3e-4\n",
    "vf_lr = 1e-3\n",
    "epsilon = 1e-2\n",
    "\n",
    "# Buffer\n",
    "gamma = 0.99\n",
    "lam = 0.95\n",
    "\n",
    "# Update\n",
    "train_pi_iters = 100\n",
    "train_v_iters = 100\n",
    "target_kl = 0.01\n",
    "epochs = 1000\n",
    "max_ep_len = 1000\n",
    "\n",
    "# Worker \n",
    "n_cpu = n_workers = 5\n",
    "total_steps,evaluate_every,print_every = 1000,50,10\n",
    "ep_len_rollout = 1000\n",
    "batch_size = 4096\n",
    "\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Ready. odim:[28] adim:[8].\n"
     ]
    }
   ],
   "source": [
    "RENDER = False\n",
    "eval_env = get_env(RENDER=RENDER)\n",
    "adim,odim = eval_env.action_space.shape[0],eval_env.observation_space.shape[0]\n",
    "print (\"Environment Ready. odim:[%d] adim:[%d].\"%(odim,adim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-23 11:39:11,454\tINFO resource_spec.py:231 -- Starting Ray with 37.01 GiB memory available for workers and up to 18.51 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-08-23 11:39:11,920\tINFO services.py:1193 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAY initialized with [5] cpus and [5] workers.\n"
     ]
    }
   ],
   "source": [
    "ray.init(num_cpus=n_cpu)\n",
    "tf.reset_default_graph()\n",
    "R = RolloutWorkerClass(seed=0)\n",
    "workers = [RayRolloutWorkerClass.remote(worker_id=i,ep_len_rollout=ep_len_rollout) \n",
    "           for i in range(n_workers)]\n",
    "print (\"RAY initialized with [%d] cpus and [%d] workers.\"%\n",
    "       (n_cpu,n_workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=26958)\u001b[0m Worker [3] initialized.\n",
      "\u001b[2m\u001b[36m(pid=26954)\u001b[0m Worker [1] initialized.\n",
      "\u001b[2m\u001b[36m(pid=26957)\u001b[0m Worker [4] initialized.\n",
      "\u001b[2m\u001b[36m(pid=26955)\u001b[0m Worker [0] initialized.\n",
      "\u001b[2m\u001b[36m(pid=26956)\u001b[0m Worker [2] initialized.\n",
      "[1/1000] pi_iter:[99/100] kl:[0.007] target_kl:[0.010] pi_loss:[-0.008] entropy:[7.31].\n",
      " sec_rollout:[3.37] sec_update:[0.53].\n",
      "[Eval. start] step:[1/1000][0.0%] #step:[0.0e+00] time:[00:00:03] ram:[5.4%].\n",
      "[Evaluate] ep_ret:[429.5945] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[10/1000] pi_iter:[99/100] kl:[0.008] target_kl:[0.010] pi_loss:[-0.011] entropy:[7.29].\n",
      " sec_rollout:[1.81] sec_update:[0.35].\n",
      "[20/1000] pi_iter:[99/100] kl:[0.009] target_kl:[0.010] pi_loss:[-0.017] entropy:[7.12].\n",
      " sec_rollout:[1.81] sec_update:[0.29].\n",
      "[30/1000] pi_iter:[99/100] kl:[0.009] target_kl:[0.010] pi_loss:[-0.018] entropy:[6.93].\n",
      " sec_rollout:[1.80] sec_update:[0.29].\n",
      "[40/1000] pi_iter:[99/100] kl:[0.012] target_kl:[0.010] pi_loss:[-0.021] entropy:[6.72].\n",
      " sec_rollout:[1.79] sec_update:[0.33].\n",
      "[50/1000] pi_iter:[99/100] kl:[0.010] target_kl:[0.010] pi_loss:[-0.016] entropy:[6.67].\n",
      " sec_rollout:[1.81] sec_update:[0.28].\n",
      "[Eval. start] step:[50/1000][4.9%] #step:[0.0e+00] time:[00:01:47] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[725.4009] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[60/1000] pi_iter:[99/100] kl:[0.013] target_kl:[0.010] pi_loss:[-0.018] entropy:[6.52].\n",
      " sec_rollout:[1.78] sec_update:[0.35].\n",
      "[70/1000] pi_iter:[11/100] kl:[0.015] target_kl:[0.010] pi_loss:[0.009] entropy:[6.31].\n",
      " sec_rollout:[1.78] sec_update:[0.14].\n",
      "[80/1000] pi_iter:[67/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.019] entropy:[6.06].\n",
      " sec_rollout:[1.77] sec_update:[0.23].\n",
      "[90/1000] pi_iter:[33/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.030] entropy:[5.88].\n",
      " sec_rollout:[1.77] sec_update:[0.17].\n",
      "[100/1000] pi_iter:[99/100] kl:[0.011] target_kl:[0.010] pi_loss:[-0.032] entropy:[5.61].\n",
      " sec_rollout:[1.76] sec_update:[0.31].\n",
      "[Eval. start] step:[100/1000][9.9%] #step:[0.0e+00] time:[00:03:32] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[996.1069] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[110/1000] pi_iter:[58/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.023] entropy:[5.52].\n",
      " sec_rollout:[1.76] sec_update:[0.27].\n",
      "[120/1000] pi_iter:[87/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.036] entropy:[5.51].\n",
      " sec_rollout:[1.76] sec_update:[0.26].\n",
      "[130/1000] pi_iter:[99/100] kl:[0.007] target_kl:[0.010] pi_loss:[-0.037] entropy:[5.33].\n",
      " sec_rollout:[1.77] sec_update:[0.30].\n",
      "[140/1000] pi_iter:[67/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.027] entropy:[5.31].\n",
      " sec_rollout:[1.79] sec_update:[0.23].\n",
      "[150/1000] pi_iter:[99/100] kl:[0.009] target_kl:[0.010] pi_loss:[-0.027] entropy:[5.22].\n",
      " sec_rollout:[1.78] sec_update:[0.31].\n",
      "[Eval. start] step:[150/1000][14.9%] #step:[0.0e+00] time:[00:05:16] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[1193.5741] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[160/1000] pi_iter:[99/100] kl:[0.011] target_kl:[0.010] pi_loss:[-0.017] entropy:[5.16].\n",
      " sec_rollout:[1.78] sec_update:[0.30].\n",
      "[170/1000] pi_iter:[99/100] kl:[0.007] target_kl:[0.010] pi_loss:[-0.025] entropy:[5.02].\n",
      " sec_rollout:[1.77] sec_update:[0.30].\n",
      "[180/1000] pi_iter:[9/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.019] entropy:[5.08].\n",
      " sec_rollout:[1.77] sec_update:[0.13].\n",
      "[190/1000] pi_iter:[51/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.029] entropy:[5.00].\n",
      " sec_rollout:[1.75] sec_update:[0.23].\n",
      "[200/1000] pi_iter:[99/100] kl:[0.011] target_kl:[0.010] pi_loss:[-0.038] entropy:[4.98].\n",
      " sec_rollout:[1.75] sec_update:[0.27].\n",
      "[Eval. start] step:[200/1000][19.9%] #step:[0.0e+00] time:[00:06:59] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[1534.6248] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[210/1000] pi_iter:[99/100] kl:[0.012] target_kl:[0.010] pi_loss:[-0.041] entropy:[4.80].\n",
      " sec_rollout:[1.75] sec_update:[0.31].\n",
      "[220/1000] pi_iter:[99/100] kl:[0.012] target_kl:[0.010] pi_loss:[-0.026] entropy:[4.62].\n",
      " sec_rollout:[1.75] sec_update:[0.36].\n",
      "[230/1000] pi_iter:[6/100] kl:[0.017] target_kl:[0.010] pi_loss:[-0.007] entropy:[4.49].\n",
      " sec_rollout:[1.75] sec_update:[0.13].\n",
      "[240/1000] pi_iter:[6/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.003] entropy:[4.45].\n",
      " sec_rollout:[1.75] sec_update:[0.15].\n",
      "[250/1000] pi_iter:[6/100] kl:[0.017] target_kl:[0.010] pi_loss:[-0.017] entropy:[4.39].\n",
      " sec_rollout:[1.75] sec_update:[0.14].\n",
      "[Eval. start] step:[250/1000][24.9%] #step:[0.0e+00] time:[00:08:42] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[1836.4692] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[260/1000] pi_iter:[99/100] kl:[0.011] target_kl:[0.010] pi_loss:[-0.031] entropy:[4.45].\n",
      " sec_rollout:[1.76] sec_update:[0.31].\n",
      "[270/1000] pi_iter:[21/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.011] entropy:[4.21].\n",
      " sec_rollout:[1.75] sec_update:[0.15].\n",
      "[280/1000] pi_iter:[30/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.025] entropy:[4.24].\n",
      " sec_rollout:[1.74] sec_update:[0.17].\n",
      "[290/1000] pi_iter:[7/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.015] entropy:[4.09].\n",
      " sec_rollout:[1.75] sec_update:[0.13].\n",
      "[300/1000] pi_iter:[7/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.004] entropy:[3.97].\n",
      " sec_rollout:[1.76] sec_update:[0.14].\n",
      "[Eval. start] step:[300/1000][29.9%] #step:[0.0e+00] time:[00:10:22] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[1844.1041] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[310/1000] pi_iter:[6/100] kl:[0.017] target_kl:[0.010] pi_loss:[-0.007] entropy:[3.94].\n",
      " sec_rollout:[1.75] sec_update:[0.13].\n",
      "[320/1000] pi_iter:[99/100] kl:[0.010] target_kl:[0.010] pi_loss:[-0.021] entropy:[3.89].\n",
      " sec_rollout:[1.77] sec_update:[0.30].\n",
      "[330/1000] pi_iter:[5/100] kl:[0.016] target_kl:[0.010] pi_loss:[0.003] entropy:[3.84].\n",
      " sec_rollout:[1.77] sec_update:[0.12].\n",
      "[340/1000] pi_iter:[60/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.028] entropy:[3.77].\n",
      " sec_rollout:[1.76] sec_update:[0.22].\n",
      "[350/1000] pi_iter:[99/100] kl:[0.009] target_kl:[0.010] pi_loss:[-0.036] entropy:[3.69].\n",
      " sec_rollout:[1.81] sec_update:[0.27].\n",
      "[Eval. start] step:[350/1000][34.9%] #step:[0.0e+00] time:[00:12:01] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[2023.4526] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[360/1000] pi_iter:[11/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.010] entropy:[3.57].\n",
      " sec_rollout:[1.76] sec_update:[0.15].\n",
      "[370/1000] pi_iter:[4/100] kl:[0.020] target_kl:[0.010] pi_loss:[0.002] entropy:[3.61].\n",
      " sec_rollout:[1.76] sec_update:[0.12].\n",
      "[380/1000] pi_iter:[99/100] kl:[0.013] target_kl:[0.010] pi_loss:[-0.025] entropy:[3.59].\n",
      " sec_rollout:[1.75] sec_update:[0.32].\n",
      "[390/1000] pi_iter:[84/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.022] entropy:[3.66].\n",
      " sec_rollout:[1.77] sec_update:[0.29].\n",
      "[400/1000] pi_iter:[99/100] kl:[0.009] target_kl:[0.010] pi_loss:[-0.036] entropy:[3.64].\n",
      " sec_rollout:[1.74] sec_update:[0.28].\n",
      "[Eval. start] step:[400/1000][39.9%] #step:[0.0e+00] time:[00:13:40] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[1941.4763] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[410/1000] pi_iter:[48/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.032] entropy:[3.62].\n",
      " sec_rollout:[1.75] sec_update:[0.21].\n",
      "[420/1000] pi_iter:[99/100] kl:[0.013] target_kl:[0.010] pi_loss:[-0.033] entropy:[3.59].\n",
      " sec_rollout:[1.83] sec_update:[0.30].\n",
      "[430/1000] pi_iter:[7/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.003] entropy:[3.53].\n",
      " sec_rollout:[1.79] sec_update:[0.13].\n",
      "[440/1000] pi_iter:[99/100] kl:[0.007] target_kl:[0.010] pi_loss:[-0.029] entropy:[3.55].\n",
      " sec_rollout:[1.75] sec_update:[0.28].\n",
      "[450/1000] pi_iter:[75/100] kl:[0.017] target_kl:[0.010] pi_loss:[-0.016] entropy:[3.48].\n",
      " sec_rollout:[1.76] sec_update:[0.25].\n",
      "[Eval. start] step:[450/1000][44.9%] #step:[0.0e+00] time:[00:15:20] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[1987.9398] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[460/1000] pi_iter:[12/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.011] entropy:[3.51].\n",
      " sec_rollout:[1.75] sec_update:[0.13].\n",
      "[470/1000] pi_iter:[99/100] kl:[0.013] target_kl:[0.010] pi_loss:[-0.026] entropy:[3.47].\n",
      " sec_rollout:[1.79] sec_update:[0.31].\n",
      "[480/1000] pi_iter:[99/100] kl:[0.012] target_kl:[0.010] pi_loss:[-0.032] entropy:[3.43].\n",
      " sec_rollout:[1.74] sec_update:[0.28].\n",
      "[490/1000] pi_iter:[33/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.009] entropy:[3.37].\n",
      " sec_rollout:[1.74] sec_update:[0.23].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500/1000] pi_iter:[15/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.001] entropy:[3.36].\n",
      " sec_rollout:[1.76] sec_update:[0.14].\n",
      "[Eval. start] step:[500/1000][49.9%] #step:[0.0e+00] time:[00:16:59] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[2105.5995] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[510/1000] pi_iter:[6/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.009] entropy:[3.36].\n",
      " sec_rollout:[1.79] sec_update:[0.13].\n",
      "[520/1000] pi_iter:[5/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.001] entropy:[3.30].\n",
      " sec_rollout:[1.78] sec_update:[0.15].\n",
      "[530/1000] pi_iter:[47/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.023] entropy:[3.14].\n",
      " sec_rollout:[1.76] sec_update:[0.24].\n",
      "[540/1000] pi_iter:[6/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.009] entropy:[3.07].\n",
      " sec_rollout:[1.75] sec_update:[0.13].\n",
      "[550/1000] pi_iter:[5/100] kl:[0.016] target_kl:[0.010] pi_loss:[0.001] entropy:[3.12].\n",
      " sec_rollout:[1.77] sec_update:[0.13].\n",
      "[Eval. start] step:[550/1000][54.9%] #step:[0.0e+00] time:[00:18:39] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[2213.3134] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[560/1000] pi_iter:[4/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.011] entropy:[3.06].\n",
      " sec_rollout:[1.76] sec_update:[0.12].\n",
      "[570/1000] pi_iter:[99/100] kl:[0.013] target_kl:[0.010] pi_loss:[-0.029] entropy:[3.06].\n",
      " sec_rollout:[1.75] sec_update:[0.32].\n",
      "[580/1000] pi_iter:[4/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.004] entropy:[3.03].\n",
      " sec_rollout:[1.76] sec_update:[0.14].\n",
      "[590/1000] pi_iter:[17/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.003] entropy:[2.98].\n",
      " sec_rollout:[1.77] sec_update:[0.19].\n",
      "[600/1000] pi_iter:[6/100] kl:[0.018] target_kl:[0.010] pi_loss:[-0.017] entropy:[2.94].\n",
      " sec_rollout:[1.74] sec_update:[0.13].\n",
      "[Eval. start] step:[600/1000][59.9%] #step:[0.0e+00] time:[00:20:17] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[2193.0307] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[610/1000] pi_iter:[4/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.013] entropy:[2.87].\n",
      " sec_rollout:[1.76] sec_update:[0.13].\n",
      "[620/1000] pi_iter:[10/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.004] entropy:[2.93].\n",
      " sec_rollout:[1.79] sec_update:[0.14].\n",
      "[630/1000] pi_iter:[5/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.003] entropy:[2.81].\n",
      " sec_rollout:[1.74] sec_update:[0.13].\n",
      "[640/1000] pi_iter:[47/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.024] entropy:[2.79].\n",
      " sec_rollout:[1.77] sec_update:[0.20].\n",
      "[650/1000] pi_iter:[5/100] kl:[0.020] target_kl:[0.010] pi_loss:[-0.005] entropy:[2.84].\n",
      " sec_rollout:[1.73] sec_update:[0.13].\n",
      "[Eval. start] step:[650/1000][64.9%] #step:[0.0e+00] time:[00:21:54] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[2123.0671] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[660/1000] pi_iter:[6/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.003] entropy:[2.76].\n",
      " sec_rollout:[1.74] sec_update:[0.13].\n",
      "[670/1000] pi_iter:[5/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.013] entropy:[2.74].\n",
      " sec_rollout:[1.77] sec_update:[0.13].\n",
      "[680/1000] pi_iter:[29/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.022] entropy:[2.80].\n",
      " sec_rollout:[1.75] sec_update:[0.18].\n",
      "[690/1000] pi_iter:[13/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.013] entropy:[2.75].\n",
      " sec_rollout:[1.73] sec_update:[0.15].\n",
      "[700/1000] pi_iter:[5/100] kl:[0.021] target_kl:[0.010] pi_loss:[-0.006] entropy:[2.80].\n",
      " sec_rollout:[1.72] sec_update:[0.12].\n",
      "[Eval. start] step:[700/1000][69.9%] #step:[0.0e+00] time:[00:23:31] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[2073.0415] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[710/1000] pi_iter:[18/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.011] entropy:[2.80].\n",
      " sec_rollout:[1.75] sec_update:[0.15].\n",
      "[720/1000] pi_iter:[17/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.005] entropy:[2.70].\n",
      " sec_rollout:[1.75] sec_update:[0.15].\n",
      "[730/1000] pi_iter:[99/100] kl:[0.009] target_kl:[0.010] pi_loss:[-0.033] entropy:[2.69].\n",
      " sec_rollout:[1.74] sec_update:[0.27].\n",
      "[740/1000] pi_iter:[5/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.000] entropy:[2.61].\n",
      " sec_rollout:[1.74] sec_update:[0.14].\n",
      "[750/1000] pi_iter:[13/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.012] entropy:[2.63].\n",
      " sec_rollout:[1.80] sec_update:[0.15].\n",
      "[Eval. start] step:[750/1000][74.9%] #step:[0.0e+00] time:[00:25:08] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[2348.1645] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[760/1000] pi_iter:[9/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.006] entropy:[2.52].\n",
      " sec_rollout:[1.74] sec_update:[0.13].\n",
      "[770/1000] pi_iter:[5/100] kl:[0.020] target_kl:[0.010] pi_loss:[-0.013] entropy:[2.43].\n",
      " sec_rollout:[1.73] sec_update:[0.13].\n",
      "[780/1000] pi_iter:[99/100] kl:[0.010] target_kl:[0.010] pi_loss:[-0.038] entropy:[2.35].\n",
      " sec_rollout:[1.74] sec_update:[0.33].\n",
      "[790/1000] pi_iter:[5/100] kl:[0.017] target_kl:[0.010] pi_loss:[-0.002] entropy:[2.41].\n",
      " sec_rollout:[1.72] sec_update:[0.13].\n",
      "[800/1000] pi_iter:[99/100] kl:[0.011] target_kl:[0.010] pi_loss:[-0.033] entropy:[2.44].\n",
      " sec_rollout:[1.73] sec_update:[0.32].\n",
      "[Eval. start] step:[800/1000][79.9%] #step:[0.0e+00] time:[00:26:46] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[2489.7569] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[810/1000] pi_iter:[6/100] kl:[0.016] target_kl:[0.010] pi_loss:[0.001] entropy:[2.46].\n",
      " sec_rollout:[1.72] sec_update:[0.12].\n",
      "[820/1000] pi_iter:[25/100] kl:[0.017] target_kl:[0.010] pi_loss:[-0.017] entropy:[2.38].\n",
      " sec_rollout:[1.74] sec_update:[0.16].\n",
      "[830/1000] pi_iter:[99/100] kl:[0.008] target_kl:[0.010] pi_loss:[-0.032] entropy:[2.42].\n",
      " sec_rollout:[1.72] sec_update:[0.27].\n",
      "[840/1000] pi_iter:[5/100] kl:[0.017] target_kl:[0.010] pi_loss:[-0.010] entropy:[2.42].\n",
      " sec_rollout:[1.75] sec_update:[0.13].\n",
      "[850/1000] pi_iter:[99/100] kl:[0.011] target_kl:[0.010] pi_loss:[-0.027] entropy:[2.38].\n",
      " sec_rollout:[1.71] sec_update:[0.31].\n",
      "[Eval. start] step:[850/1000][84.9%] #step:[0.0e+00] time:[00:28:23] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[2412.3164] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[860/1000] pi_iter:[6/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.006] entropy:[2.42].\n",
      " sec_rollout:[1.72] sec_update:[0.13].\n",
      "[870/1000] pi_iter:[16/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.005] entropy:[2.44].\n",
      " sec_rollout:[1.72] sec_update:[0.15].\n",
      "[880/1000] pi_iter:[5/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.013] entropy:[2.38].\n",
      " sec_rollout:[1.78] sec_update:[0.13].\n",
      "[890/1000] pi_iter:[8/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.011] entropy:[2.28].\n",
      " sec_rollout:[1.72] sec_update:[0.14].\n",
      "[900/1000] pi_iter:[5/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.006] entropy:[2.21].\n",
      " sec_rollout:[1.73] sec_update:[0.13].\n",
      "[Eval. start] step:[900/1000][89.9%] #step:[0.0e+00] time:[00:30:01] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[2389.4116] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[910/1000] pi_iter:[29/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.017] entropy:[2.26].\n",
      " sec_rollout:[1.73] sec_update:[0.17].\n",
      "[920/1000] pi_iter:[99/100] kl:[0.012] target_kl:[0.010] pi_loss:[-0.038] entropy:[2.19].\n",
      " sec_rollout:[1.76] sec_update:[0.27].\n",
      "[930/1000] pi_iter:[4/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.015] entropy:[2.11].\n",
      " sec_rollout:[1.79] sec_update:[0.14].\n",
      "[940/1000] pi_iter:[6/100] kl:[0.019] target_kl:[0.010] pi_loss:[-0.010] entropy:[2.06].\n",
      " sec_rollout:[1.75] sec_update:[0.13].\n",
      "[950/1000] pi_iter:[13/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.015] entropy:[2.03].\n",
      " sec_rollout:[1.71] sec_update:[0.14].\n",
      "[Eval. start] step:[950/1000][94.9%] #step:[0.0e+00] time:[00:31:38] ram:[5.5%].\n",
      "[Evaluate] ep_ret:[2538.8009] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "[960/1000] pi_iter:[4/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.003] entropy:[1.99].\n",
      " sec_rollout:[1.74] sec_update:[0.14].\n",
      "[970/1000] pi_iter:[6/100] kl:[0.015] target_kl:[0.010] pi_loss:[0.005] entropy:[2.13].\n",
      " sec_rollout:[1.72] sec_update:[0.13].\n",
      "[980/1000] pi_iter:[39/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.021] entropy:[2.01].\n",
      " sec_rollout:[1.73] sec_update:[0.22].\n",
      "[990/1000] pi_iter:[25/100] kl:[0.016] target_kl:[0.010] pi_loss:[-0.006] entropy:[2.03].\n",
      " sec_rollout:[1.74] sec_update:[0.16].\n",
      "[1000/1000] pi_iter:[11/100] kl:[0.015] target_kl:[0.010] pi_loss:[-0.012] entropy:[2.11].\n",
      " sec_rollout:[1.74] sec_update:[0.15].\n",
      "[Eval. start] step:[1000/1000][99.9%] #step:[0.0e+00] time:[00:33:14] ram:[5.5%].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluate] ep_ret:[2532.0488] ep_len:[1000]\n",
      "[../net/ppo_ant/model.npz] saved.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "n_env_step = 0 # number of environment steps\n",
    "for t in range(int(total_steps)):\n",
    "    esec = time.time()-start_time\n",
    "    \n",
    "    # 1. Synchronize worker weights\n",
    "    weights = R.get_weights()\n",
    "    set_weights_list = [worker.set_weights.remote(weights) for worker in workers] \n",
    "    \n",
    "    # 2. Make rollout and accumulate to Buffers\n",
    "    t_start = time.time()\n",
    "    ops = [worker.rollout.remote() for worker in workers]\n",
    "    rollout_vals = ray.get(ops)\n",
    "    sec_rollout = time.time() - t_start\n",
    "    \n",
    "    # 3. Update\n",
    "    t_start = time.time() # tic\n",
    "    # Mini-batch type of update\n",
    "    for r_idx,rval in enumerate(rollout_vals):\n",
    "        obs_buf,act_buf,adv_buf,ret_buf,logp_buf = \\\n",
    "            rval[0],rval[1],rval[2],rval[3],rval[4]\n",
    "        if r_idx == 0:\n",
    "            obs_bufs,act_bufs,adv_bufs,ret_bufs,logp_bufs = \\\n",
    "                obs_buf,act_buf,adv_buf,ret_buf,logp_buf\n",
    "        else:\n",
    "            obs_bufs = np.concatenate((obs_bufs,obs_buf),axis=0)\n",
    "            act_bufs = np.concatenate((act_bufs,act_buf),axis=0)\n",
    "            adv_bufs = np.concatenate((adv_bufs,adv_buf),axis=0)\n",
    "            ret_bufs = np.concatenate((ret_bufs,ret_buf),axis=0)\n",
    "            logp_bufs = np.concatenate((logp_bufs,logp_buf),axis=0)\n",
    "    n_val_total = obs_bufs.shape[0]\n",
    "    for pi_iter in range(train_pi_iters):\n",
    "        rand_idx = np.random.permutation(n_val_total)[:batch_size]\n",
    "        buf_batches = [obs_bufs[rand_idx],act_bufs[rand_idx],adv_bufs[rand_idx],\n",
    "                       ret_bufs[rand_idx],logp_bufs[rand_idx]]\n",
    "        feeds = {k:v for k,v in zip(R.model['all_phs'],buf_batches)}\n",
    "        _,kl,pi_loss,ent = R.sess.run([R.graph['train_pi'],R.graph['approx_kl'],\n",
    "                               R.graph['pi_loss'],R.graph['approx_ent']],\n",
    "                           feed_dict=feeds)        \n",
    "        if kl > 1.5 * target_kl:\n",
    "            # print (\"  pi_iter:[%d] kl(%.3f) is higher than 1.5x(%.3f)\"%(pi_iter,kl,target_kl))\n",
    "            break\n",
    "    for _ in range(train_v_iters):\n",
    "        rand_idx = np.random.permutation(n_val_total)[:batch_size]\n",
    "        buf_batches = [obs_bufs[rand_idx],act_bufs[rand_idx],adv_bufs[rand_idx],\n",
    "                       ret_bufs[rand_idx],logp_bufs[rand_idx]]\n",
    "        feeds = {k:v for k,v in zip(R.model['all_phs'],buf_batches)}\n",
    "        R.sess.run(R.graph['train_v'],feed_dict=feeds)\n",
    "    sec_update = time.time() - t_start # toc\n",
    "    \n",
    "    # Print\n",
    "    if (t == 0) or (((t+1)%print_every) == 0): \n",
    "        print (\"[%d/%d] pi_iter:[%d/%d] kl:[%.3f] target_kl:[%.3f] pi_loss:[%.3f] entropy:[%.2f].\"%\n",
    "               (t+1,total_steps,pi_iter,train_pi_iters,kl,target_kl,pi_loss,ent))\n",
    "        print (\" sec_rollout:[%.2f] sec_update:[%.2f].\"%(sec_rollout,sec_update))\n",
    "        \n",
    "    # Evaluate\n",
    "    if (t==0) or (((t+1)%evaluate_every) == 0):\n",
    "        ram_percent = psutil.virtual_memory().percent # memory usage\n",
    "        print (\"[Eval. start] step:[%d/%d][%.1f%%] #step:[%.1e] time:[%s] ram:[%.1f%%].\"%\n",
    "               (t+1,total_steps,t/total_steps*100,\n",
    "                n_env_step,\n",
    "                time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time)),\n",
    "                ram_percent)\n",
    "              )\n",
    "        o,d,ep_ret,ep_len = eval_env.reset(),False,0,0\n",
    "        if RENDER:\n",
    "            _ = eval_env.render(mode='human') \n",
    "        while not(d or (ep_len == max_ep_len)):\n",
    "            a = R.sess.run(R.model['mu'],feed_dict={R.model['o_ph']:o.reshape(1,-1)})\n",
    "            o,r,d,_ = eval_env.step(a[0])\n",
    "            if RENDER:\n",
    "                _ = eval_env.render(mode='human') \n",
    "            ep_ret += r # compute return \n",
    "            ep_len += 1\n",
    "        print (\"[Evaluate] ep_ret:[%.4f] ep_len:[%d]\"%(ep_ret,ep_len))\n",
    "        \n",
    "        # Save \n",
    "        npz_path = '../net/ppo_ant/model.npz'\n",
    "        save_ppo_model(npz_path,R,VERBOSE=False)\n",
    "\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env and ray closed.\n"
     ]
    }
   ],
   "source": [
    "eval_env.close()\n",
    "ray.shutdown()\n",
    "print (\"Env and ray closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network initialized.\n"
     ]
    }
   ],
   "source": [
    "R.sess.run(tf.global_variables_initializer())\n",
    "print (\"Network initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[../net/ppo_ant/model.npz] loaded.\n"
     ]
    }
   ],
   "source": [
    "npz_path = '../net/ppo_ant/model.npz'\n",
    "restore_ppo_model(npz_path,R,VERBOSE=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Evaluate] ep_ret:[2573.5052] ep_len:[1000]\n"
     ]
    }
   ],
   "source": [
    "RENDER = False\n",
    "eval_env = get_env(RENDER=RENDER)\n",
    "o,d,ep_ret,ep_len = eval_env.reset(),False,0,0\n",
    "if RENDER:\n",
    "    _ = eval_env.render(mode='human') \n",
    "while not(d or (ep_len == max_ep_len)):\n",
    "    a = R.sess.run(R.model['mu'],feed_dict={R.model['o_ph']:o.reshape(1,-1)})\n",
    "    o,r,d,_ = eval_env.step(a[0])\n",
    "    if RENDER:\n",
    "        _ = eval_env.render(mode='human') \n",
    "    ep_ret += r # compute return \n",
    "    ep_len += 1\n",
    "print (\"[Evaluate] ep_ret:[%.4f] ep_len:[%d]\"%(ep_ret,ep_len))\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
