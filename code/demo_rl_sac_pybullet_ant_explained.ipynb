{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor Critic on PyBullet Ant explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packaged loaded. TF version is [1.15.0].\n"
     ]
    }
   ],
   "source": [
    "import datetime,gym,os,pybullet_envs,time,os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.set_printoptions(precision=2)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "print (\"Packaged loaded. TF version is [%s].\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer for SAC agents.\n",
    "    \"\"\"\n",
    "    def __init__(self, odim, adim, size):\n",
    "        self.obs1_buf = np.zeros([size, odim], dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros([size, odim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size, adim], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs1_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        return dict(obs1=self.obs1_buf[idxs],\n",
    "                    obs2=self.obs2_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs],\n",
    "                    done=self.done_buf[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Actor Critic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "def create_sac_model(odim=10,adim=2,hdims=[256,256]):\n",
    "    \"\"\"\n",
    "    Soft Actor Critic Model (compatible with Ray)\n",
    "    \"\"\"\n",
    "    import tensorflow as tf # make it compatible with Ray actors\n",
    "    \n",
    "    def mlp(x,hdims=[256,256],actv=tf.nn.relu,out_actv=tf.nn.relu):\n",
    "        ki = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        for hdim in hdims[:-1]:\n",
    "            x = tf.layers.dense(x,units=hdim,activation=actv,kernel_initializer=ki)\n",
    "        return tf.layers.dense(x,units=hdims[-1],activation=out_actv,kernel_initializer=ki)\n",
    "    def gaussian_loglik(x,mu,log_std):\n",
    "        EPS = 1e-8\n",
    "        pre_sum = -0.5*(\n",
    "            ( (x-mu)/(tf.exp(log_std)+EPS) )**2 +\n",
    "            2*log_std + np.log(2*np.pi)\n",
    "        )\n",
    "        return tf.reduce_sum(pre_sum, axis=1)\n",
    "    def mlp_gaussian_policy(o,adim=2,hdims=[256,256],actv=tf.nn.relu):\n",
    "        net = mlp(x=o,hdims=hdims,actv=actv,out_actv=actv) # feature \n",
    "        mu = tf.layers.dense(net,adim,activation=None) # mu\n",
    "        log_std = tf.layers.dense(net,adim,activation=None) # log_std\n",
    "        LOG_STD_MIN,LOG_STD_MAX = -10.0,+2.0\n",
    "        log_std = tf.clip_by_value(log_std, LOG_STD_MIN, LOG_STD_MAX) \n",
    "        std = tf.exp(log_std) # std \n",
    "        pi = mu + tf.random_normal(tf.shape(mu)) * std  # sampled\n",
    "        logp_pi = gaussian_loglik(x=pi,mu=mu,log_std=log_std) # log lik\n",
    "        return mu,pi,logp_pi\n",
    "    def squash_action(mu,pi,logp_pi):\n",
    "        # Squash those unbounded actions\n",
    "        logp_pi -= tf.reduce_sum(2*(np.log(2) - pi -\n",
    "                                    tf.nn.softplus(-2*pi)), axis=1)\n",
    "        mu,pi = tf.tanh(mu),tf.tanh(pi)\n",
    "        return mu, pi, logp_pi\n",
    "    def mlp_actor_critic(o,a,hdims=[256,256],actv=tf.nn.relu,out_actv=None,\n",
    "                         policy=mlp_gaussian_policy):\n",
    "        adim = a.shape.as_list()[-1]\n",
    "        with tf.variable_scope('pi'): # policy\n",
    "            mu,pi,logp_pi = policy(o=o,adim=adim,hdims=hdims,actv=actv)\n",
    "            mu,pi,logp_pi = squash_action(mu=mu,pi=pi,logp_pi=logp_pi)\n",
    "        def vf_mlp(x): return tf.squeeze(\n",
    "            mlp(x=x,hdims=hdims+[1],actv=actv,out_actv=None),axis=1)\n",
    "        with tf.variable_scope('q1'): q1 = vf_mlp( tf.concat([o,a],axis=-1))\n",
    "        with tf.variable_scope('q2'): q2 = vf_mlp( tf.concat([o,a],axis=-1))\n",
    "        return mu,pi,logp_pi,q1,q2\n",
    "    \n",
    "    def placeholder(dim=None):\n",
    "        return tf.placeholder(dtype=tf.float32,shape=(None,dim) if dim else (None,))\n",
    "    def placeholders(*args):\n",
    "        \"\"\"\n",
    "        Usage: a_ph,b_ph,c_ph = placeholders(adim,bdim,None)\n",
    "        \"\"\"\n",
    "        return [placeholder(dim) for dim in args]\n",
    "    def get_vars(scope):\n",
    "        return [x for x in tf.compat.v1.global_variables() if scope in x.name]\n",
    "    \n",
    "    # Have own session\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    \n",
    "    # Placeholders\n",
    "    o_ph,a_ph,o2_ph,r_ph,d_ph = placeholders(odim,adim,odim,None,None)\n",
    "    # Actor critic \n",
    "    ac_kwargs = {'hdims':hdims,'actv':tf.nn.relu,'out_actv':None,'policy':mlp_gaussian_policy}\n",
    "    with tf.variable_scope('main'):\n",
    "        mu,pi,logp_pi,q1,q2 = mlp_actor_critic(o=o_ph,a=a_ph,**ac_kwargs)\n",
    "    with tf.variable_scope('main',reuse=True):\n",
    "        _,_,_,q1_pi,q2_pi = mlp_actor_critic(o=o_ph,a=pi,**ac_kwargs)\n",
    "        _,pi_next,logp_pi_next,_,_ = mlp_actor_critic(o=o2_ph,a=a_ph,**ac_kwargs)\n",
    "    # Target value\n",
    "    with tf.variable_scope('target'):\n",
    "        _,_,_,q1_targ,q2_targ = mlp_actor_critic(o=o2_ph,a=pi_next,**ac_kwargs)\n",
    "        \n",
    "    # Get variables\n",
    "    main_vars,q_vars,pi_vars,target_vars = \\\n",
    "        get_vars('main'),get_vars('main/q'),get_vars('main/pi'),get_vars('target')\n",
    "    \n",
    "    model = {'o_ph':o_ph,'a_ph':a_ph,'o2_ph':o2_ph,'r_ph':r_ph,'d_ph':d_ph,\n",
    "             'mu':mu,'pi':pi,'logp_pi':logp_pi,'q1':q1,'q2':q2,\n",
    "             'q1_pi':q1_pi,'q2_pi':q2_pi,\n",
    "             'pi_next':pi_next,'logp_pi_next':logp_pi_next,\n",
    "             'q1_targ':q1_targ,'q2_targ':q2_targ,\n",
    "             'main_vars':main_vars,'q_vars':q_vars,'pi_vars':pi_vars,'target_vars':target_vars}\n",
    "        \n",
    "    return model,sess\n",
    "\n",
    "def create_sac_graph(model,lr=1e-3,gamma=0.98,alpha=0.1,polyak=0.995):\n",
    "    \"\"\"\n",
    "    SAC Computational Graph\n",
    "    \"\"\"\n",
    "    # Double Q-learning\n",
    "    min_q_pi = tf.minimum(model['q1_pi'],model['q2_pi'])\n",
    "    min_q_targ = tf.minimum(model['q1_targ'],model['q2_targ'])\n",
    "    \n",
    "    # Entropy-regularized Bellman backup\n",
    "    q_backup = tf.stop_gradient(\n",
    "        model['r_ph'] + \n",
    "        gamma*(1-model['d_ph'])*(min_q_targ - alpha*model['logp_pi_next'])\n",
    "    )\n",
    "    \n",
    "    # Soft actor-critic losses\n",
    "    pi_loss = tf.reduce_mean(alpha*model['logp_pi'] - min_q_pi)\n",
    "    q1_loss = 0.5 * tf.reduce_mean((q_backup - model['q1'])**2)\n",
    "    q2_loss = 0.5 * tf.reduce_mean((q_backup - model['q2'])**2)\n",
    "    value_loss = q1_loss + q2_loss\n",
    "    \n",
    "    # Policy train op\n",
    "    pi_optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    train_pi_op = pi_optimizer.minimize(pi_loss,var_list=model['pi_vars'])\n",
    "    \n",
    "    # Value train op \n",
    "    value_optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    with tf.control_dependencies([train_pi_op]):\n",
    "        train_value_op = value_optimizer.minimize(value_loss,var_list=model['q_vars'])\n",
    "        \n",
    "    # Polyak averaging for target variables\n",
    "    with tf.control_dependencies([train_value_op]):\n",
    "        target_update = tf.group([tf.assign(v_targ, polyak*v_targ + (1-polyak)*v_main)\n",
    "                                  for v_main, v_targ in \n",
    "                                      zip(model['main_vars'], model['target_vars'])]\n",
    "                                )\n",
    "    \n",
    "    # All ops to call during one training step\n",
    "    step_ops = [pi_loss, q1_loss, q2_loss, model['q1'], model['q2'], model['logp_pi'],\n",
    "                train_pi_op, train_value_op, target_update]\n",
    "    \n",
    "    # Initializing targets to match main variables\n",
    "    target_init = tf.group([tf.assign(v_targ, v_main)\n",
    "                            for v_main, v_targ in \n",
    "                                zip(model['main_vars'], model['target_vars'])]\n",
    "                          )\n",
    "\n",
    "    return step_ops,target_init\n",
    "    \n",
    "def get_action(model,sess,o,deterministic=False):\n",
    "    act_op = model['mu'] if deterministic else model['pi']\n",
    "    return sess.run(act_op, feed_dict={model['o_ph']:o.reshape(1,-1)})[0]\n",
    "\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_RENDER = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AntBulletEnv-v0] ready.\n",
      "odim:[28] adim:[8].\n"
     ]
    }
   ],
   "source": [
    "gym.logger.set_level(40)\n",
    "env_name = 'AntBulletEnv-v0'\n",
    "env,test_env = gym.make(env_name),gym.make(env_name)\n",
    "if DO_RENDER:\n",
    "    _ = test_env.render(mode='human') # enable rendering on test_env\n",
    "_ = test_env.reset()\n",
    "for _ in range(3): # dummy run for proper rendering \n",
    "    a = test_env.action_space.sample()\n",
    "    o,r,d,_ = test_env.step(a)\n",
    "    time.sleep(0.01)\n",
    "print (\"[%s] ready.\"%(env_name))\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space # -1.0 ~ +1.0\n",
    "odim,adim = observation_space.shape[0],action_space.shape[0]\n",
    "print (\"odim:[%d] adim:[%d].\"%(odim,adim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model,sess = create_sac_model(odim=odim,adim=adim)\n",
    "step_ops,target_init = create_sac_graph(model,lr=1e-3,gamma=0.98,alpha=0.1,polyak=0.995)\n",
    "# Replay buffers\n",
    "replay_buffer = ReplayBuffer(odim=odim,adim=adim,size=int(1e6))\n",
    "replay_buffer_short = ReplayBuffer(odim=odim,adim=adim,size=int(1e5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration \n",
    "total_steps,start_steps = 1e6,1e4\n",
    "update_every,update_count,batch_size,max_ep_len_train = 1,2,128,1e3\n",
    "evaluate_every,num_eval,max_ep_len_test = 1e4,3,1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed and initialize the model\n",
    "seed = 0\n",
    "tf.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(target_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:[10000/1000000][1.0%] time:00:00:08.\n",
      " [Evaluate] [0/3] ep_ret:[415.4694] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[470.6322] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[469.1429] ep_len:[1000]\n",
      "step:[20000/1000000][2.0%] time:00:01:23.\n",
      " [Evaluate] [0/3] ep_ret:[797.6973] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[125.7220] ep_len:[174]\n",
      " [Evaluate] [2/3] ep_ret:[612.6378] ep_len:[1000]\n",
      "step:[30000/1000000][3.0%] time:00:02:36.\n",
      " [Evaluate] [0/3] ep_ret:[617.0698] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[750.5206] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[743.1780] ep_len:[1000]\n",
      "step:[40000/1000000][4.0%] time:00:03:51.\n",
      " [Evaluate] [0/3] ep_ret:[809.4288] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[859.5367] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[681.5553] ep_len:[1000]\n",
      "step:[50000/1000000][5.0%] time:00:05:05.\n",
      " [Evaluate] [0/3] ep_ret:[751.4952] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[770.4186] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[734.3855] ep_len:[1000]\n",
      "step:[60000/1000000][6.0%] time:00:06:19.\n",
      " [Evaluate] [0/3] ep_ret:[886.2916] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[950.0494] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[872.2522] ep_len:[1000]\n",
      "step:[70000/1000000][7.0%] time:00:07:34.\n",
      " [Evaluate] [0/3] ep_ret:[595.7280] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[836.5782] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[826.4031] ep_len:[1000]\n",
      "step:[80000/1000000][8.0%] time:00:08:48.\n",
      " [Evaluate] [0/3] ep_ret:[961.2166] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[958.6216] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[955.5145] ep_len:[1000]\n",
      "step:[90000/1000000][9.0%] time:00:10:02.\n",
      " [Evaluate] [0/3] ep_ret:[741.3505] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[919.1980] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[868.9089] ep_len:[1000]\n",
      "step:[100000/1000000][10.0%] time:00:11:17.\n",
      " [Evaluate] [0/3] ep_ret:[745.1899] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[976.4283] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[987.0352] ep_len:[1000]\n",
      "step:[110000/1000000][11.0%] time:00:12:31.\n",
      " [Evaluate] [0/3] ep_ret:[894.0155] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[610.3808] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[934.1117] ep_len:[1000]\n",
      "step:[120000/1000000][12.0%] time:00:13:45.\n",
      " [Evaluate] [0/3] ep_ret:[998.5801] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[963.2208] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[972.8945] ep_len:[1000]\n",
      "step:[130000/1000000][13.0%] time:00:15:00.\n",
      " [Evaluate] [0/3] ep_ret:[946.6077] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[929.3350] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[927.0574] ep_len:[1000]\n",
      "step:[140000/1000000][14.0%] time:00:16:14.\n",
      " [Evaluate] [0/3] ep_ret:[920.1480] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[967.1217] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[911.4456] ep_len:[1000]\n",
      "step:[150000/1000000][15.0%] time:00:17:28.\n",
      " [Evaluate] [0/3] ep_ret:[905.0777] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[931.2083] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[911.1588] ep_len:[1000]\n",
      "step:[160000/1000000][16.0%] time:00:18:43.\n",
      " [Evaluate] [0/3] ep_ret:[993.8873] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[916.3933] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[972.5289] ep_len:[1000]\n",
      "step:[170000/1000000][17.0%] time:00:19:57.\n",
      " [Evaluate] [0/3] ep_ret:[1011.6061] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1002.9632] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1008.5821] ep_len:[1000]\n",
      "step:[180000/1000000][18.0%] time:00:21:11.\n",
      " [Evaluate] [0/3] ep_ret:[953.2980] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[945.8879] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[905.6166] ep_len:[1000]\n",
      "step:[190000/1000000][19.0%] time:00:22:26.\n",
      " [Evaluate] [0/3] ep_ret:[843.8455] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[844.4957] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[993.1898] ep_len:[1000]\n",
      "step:[200000/1000000][20.0%] time:00:23:40.\n",
      " [Evaluate] [0/3] ep_ret:[990.6079] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[901.0996] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[988.3119] ep_len:[1000]\n",
      "step:[210000/1000000][21.0%] time:00:24:54.\n",
      " [Evaluate] [0/3] ep_ret:[934.2662] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[921.0534] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[838.8650] ep_len:[1000]\n",
      "step:[220000/1000000][22.0%] time:00:26:09.\n",
      " [Evaluate] [0/3] ep_ret:[964.3517] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[961.4480] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[964.0682] ep_len:[1000]\n",
      "step:[230000/1000000][23.0%] time:00:27:23.\n",
      " [Evaluate] [0/3] ep_ret:[1019.9034] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1012.1702] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1014.5531] ep_len:[1000]\n",
      "step:[240000/1000000][24.0%] time:00:28:37.\n",
      " [Evaluate] [0/3] ep_ret:[1065.4528] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1046.7782] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1040.5344] ep_len:[1000]\n",
      "step:[250000/1000000][25.0%] time:00:29:51.\n",
      " [Evaluate] [0/3] ep_ret:[960.9359] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[956.9790] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[947.6785] ep_len:[1000]\n",
      "step:[260000/1000000][26.0%] time:00:31:06.\n",
      " [Evaluate] [0/3] ep_ret:[1005.0268] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[998.8644] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[968.8480] ep_len:[1000]\n",
      "step:[270000/1000000][27.0%] time:00:32:20.\n",
      " [Evaluate] [0/3] ep_ret:[966.2680] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1033.2414] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1042.2421] ep_len:[1000]\n",
      "step:[280000/1000000][28.0%] time:00:33:34.\n",
      " [Evaluate] [0/3] ep_ret:[1034.9019] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[954.9529] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[979.2759] ep_len:[1000]\n",
      "step:[290000/1000000][29.0%] time:00:34:48.\n",
      " [Evaluate] [0/3] ep_ret:[957.7460] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1047.9271] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1027.5702] ep_len:[1000]\n",
      "step:[300000/1000000][30.0%] time:00:36:03.\n",
      " [Evaluate] [0/3] ep_ret:[1055.6744] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1052.6883] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1048.1866] ep_len:[1000]\n",
      "step:[310000/1000000][31.0%] time:00:37:17.\n",
      " [Evaluate] [0/3] ep_ret:[1033.9014] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1039.5638] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1031.9327] ep_len:[1000]\n",
      "step:[320000/1000000][32.0%] time:00:38:31.\n",
      " [Evaluate] [0/3] ep_ret:[1137.0429] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1119.9225] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[960.7745] ep_len:[1000]\n",
      "step:[330000/1000000][33.0%] time:00:39:45.\n",
      " [Evaluate] [0/3] ep_ret:[1035.2361] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1043.0580] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1010.1400] ep_len:[1000]\n",
      "step:[340000/1000000][34.0%] time:00:40:59.\n",
      " [Evaluate] [0/3] ep_ret:[894.3164] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[727.7135] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[792.6325] ep_len:[1000]\n",
      "step:[350000/1000000][35.0%] time:00:42:14.\n",
      " [Evaluate] [0/3] ep_ret:[1016.4279] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[966.3838] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[997.9609] ep_len:[1000]\n",
      "step:[360000/1000000][36.0%] time:00:43:28.\n",
      " [Evaluate] [0/3] ep_ret:[654.2571] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1342.6311] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1323.3168] ep_len:[1000]\n",
      "step:[370000/1000000][37.0%] time:00:44:42.\n",
      " [Evaluate] [0/3] ep_ret:[1140.9940] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1024.3313] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1090.9207] ep_len:[1000]\n",
      "step:[380000/1000000][38.0%] time:00:45:56.\n",
      " [Evaluate] [0/3] ep_ret:[1460.9925] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1453.7634] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1460.4435] ep_len:[1000]\n",
      "step:[390000/1000000][39.0%] time:00:47:10.\n",
      " [Evaluate] [0/3] ep_ret:[1376.0133] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1369.1358] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1372.6883] ep_len:[1000]\n",
      "step:[400000/1000000][40.0%] time:00:48:24.\n",
      " [Evaluate] [0/3] ep_ret:[1429.1564] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1413.0643] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1417.9197] ep_len:[1000]\n",
      "step:[410000/1000000][41.0%] time:00:49:39.\n",
      " [Evaluate] [0/3] ep_ret:[1447.5875] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1431.8724] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1333.0599] ep_len:[1000]\n",
      "step:[420000/1000000][42.0%] time:00:50:53.\n",
      " [Evaluate] [0/3] ep_ret:[1428.8216] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1424.3062] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1411.3582] ep_len:[1000]\n",
      "step:[430000/1000000][43.0%] time:00:52:07.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [Evaluate] [0/3] ep_ret:[1366.5591] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1088.2745] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1082.7815] ep_len:[1000]\n",
      "step:[440000/1000000][44.0%] time:00:53:21.\n",
      " [Evaluate] [0/3] ep_ret:[1445.4660] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1455.1183] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1454.1747] ep_len:[1000]\n",
      "step:[450000/1000000][45.0%] time:00:54:35.\n",
      " [Evaluate] [0/3] ep_ret:[1476.4282] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1481.2252] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1457.5644] ep_len:[1000]\n",
      "step:[460000/1000000][46.0%] time:00:55:49.\n",
      " [Evaluate] [0/3] ep_ret:[1451.2191] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1412.8140] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1418.6446] ep_len:[1000]\n",
      "step:[470000/1000000][47.0%] time:00:57:03.\n",
      " [Evaluate] [0/3] ep_ret:[1516.2616] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1514.2714] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1504.9514] ep_len:[1000]\n",
      "step:[480000/1000000][48.0%] time:00:58:17.\n",
      " [Evaluate] [0/3] ep_ret:[1494.0043] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1459.3141] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1479.2355] ep_len:[1000]\n",
      "step:[490000/1000000][49.0%] time:00:59:32.\n",
      " [Evaluate] [0/3] ep_ret:[1551.6118] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1552.5304] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1543.9784] ep_len:[1000]\n",
      "step:[500000/1000000][50.0%] time:01:00:46.\n",
      " [Evaluate] [0/3] ep_ret:[1617.8502] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1632.9213] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1628.7235] ep_len:[1000]\n",
      "step:[510000/1000000][51.0%] time:01:02:00.\n",
      " [Evaluate] [0/3] ep_ret:[1626.3053] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1646.8411] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1648.3099] ep_len:[1000]\n",
      "step:[520000/1000000][52.0%] time:01:03:14.\n",
      " [Evaluate] [0/3] ep_ret:[1679.9249] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1677.7813] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1667.7211] ep_len:[1000]\n",
      "step:[530000/1000000][53.0%] time:01:04:28.\n",
      " [Evaluate] [0/3] ep_ret:[1145.6147] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1172.2065] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1002.9084] ep_len:[1000]\n",
      "step:[540000/1000000][54.0%] time:01:05:42.\n",
      " [Evaluate] [0/3] ep_ret:[1802.5670] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1084.9191] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1089.0879] ep_len:[1000]\n",
      "step:[550000/1000000][55.0%] time:01:06:56.\n",
      " [Evaluate] [0/3] ep_ret:[1875.6957] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1844.3601] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1893.5573] ep_len:[1000]\n",
      "step:[560000/1000000][56.0%] time:01:08:10.\n",
      " [Evaluate] [0/3] ep_ret:[1864.5348] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1823.6385] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1862.7903] ep_len:[1000]\n",
      "step:[570000/1000000][57.0%] time:01:09:24.\n",
      " [Evaluate] [0/3] ep_ret:[1947.4254] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1962.0837] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1937.2349] ep_len:[1000]\n",
      "step:[580000/1000000][58.0%] time:01:10:38.\n",
      " [Evaluate] [0/3] ep_ret:[2068.6537] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[1988.3757] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2039.0627] ep_len:[1000]\n",
      "step:[590000/1000000][59.0%] time:01:11:52.\n",
      " [Evaluate] [0/3] ep_ret:[2089.7981] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2062.7443] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[1124.3455] ep_len:[1000]\n",
      "step:[600000/1000000][60.0%] time:01:13:06.\n",
      " [Evaluate] [0/3] ep_ret:[2200.5594] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2200.0887] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2207.5494] ep_len:[1000]\n",
      "step:[610000/1000000][61.0%] time:01:14:19.\n",
      " [Evaluate] [0/3] ep_ret:[2259.2560] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2264.6174] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2257.4535] ep_len:[1000]\n",
      "step:[620000/1000000][62.0%] time:01:15:33.\n",
      " [Evaluate] [0/3] ep_ret:[2294.5334] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2295.1895] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2315.6767] ep_len:[1000]\n",
      "step:[630000/1000000][63.0%] time:01:16:47.\n",
      " [Evaluate] [0/3] ep_ret:[2079.2945] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2138.0768] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2111.5458] ep_len:[1000]\n",
      "step:[640000/1000000][64.0%] time:01:18:01.\n",
      " [Evaluate] [0/3] ep_ret:[2170.4277] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2161.9220] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2178.1000] ep_len:[1000]\n",
      "step:[650000/1000000][65.0%] time:01:19:15.\n",
      " [Evaluate] [0/3] ep_ret:[2257.2997] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2197.1718] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2257.3893] ep_len:[1000]\n",
      "step:[660000/1000000][66.0%] time:01:20:29.\n",
      " [Evaluate] [0/3] ep_ret:[2449.2322] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2455.9255] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2451.9709] ep_len:[1000]\n",
      "step:[670000/1000000][67.0%] time:01:21:42.\n",
      " [Evaluate] [0/3] ep_ret:[2360.1767] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2359.6167] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2356.1732] ep_len:[1000]\n",
      "step:[680000/1000000][68.0%] time:01:22:56.\n",
      " [Evaluate] [0/3] ep_ret:[2427.4035] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2428.6409] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2422.6882] ep_len:[1000]\n",
      "step:[690000/1000000][69.0%] time:01:24:10.\n",
      " [Evaluate] [0/3] ep_ret:[2419.5306] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2427.3748] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2410.3386] ep_len:[1000]\n",
      "step:[700000/1000000][70.0%] time:01:25:24.\n",
      " [Evaluate] [0/3] ep_ret:[2459.4910] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2456.8112] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2466.2564] ep_len:[1000]\n",
      "step:[710000/1000000][71.0%] time:01:26:38.\n",
      " [Evaluate] [0/3] ep_ret:[2496.8377] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2493.2046] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2501.6744] ep_len:[1000]\n",
      "step:[720000/1000000][72.0%] time:01:27:52.\n",
      " [Evaluate] [0/3] ep_ret:[2536.2021] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2548.1726] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2557.2199] ep_len:[1000]\n",
      "step:[730000/1000000][73.0%] time:01:29:06.\n",
      " [Evaluate] [0/3] ep_ret:[2541.6138] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2547.3383] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2544.7636] ep_len:[1000]\n",
      "step:[740000/1000000][74.0%] time:01:30:20.\n",
      " [Evaluate] [0/3] ep_ret:[2568.9249] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2559.8674] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2566.1958] ep_len:[1000]\n",
      "step:[750000/1000000][75.0%] time:01:31:33.\n",
      " [Evaluate] [0/3] ep_ret:[2615.4207] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2608.8005] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2612.3348] ep_len:[1000]\n",
      "step:[760000/1000000][76.0%] time:01:32:47.\n",
      " [Evaluate] [0/3] ep_ret:[2572.2972] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2568.4959] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2555.9523] ep_len:[1000]\n",
      "step:[770000/1000000][77.0%] time:01:34:01.\n",
      " [Evaluate] [0/3] ep_ret:[2685.9243] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2682.5699] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2663.1795] ep_len:[1000]\n",
      "step:[780000/1000000][78.0%] time:01:35:15.\n",
      " [Evaluate] [0/3] ep_ret:[2605.4968] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2609.6981] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2601.5249] ep_len:[1000]\n",
      "step:[790000/1000000][79.0%] time:01:36:29.\n",
      " [Evaluate] [0/3] ep_ret:[2707.3234] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2710.0395] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2693.3533] ep_len:[1000]\n",
      "step:[800000/1000000][80.0%] time:01:37:43.\n",
      " [Evaluate] [0/3] ep_ret:[2645.7744] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2639.7324] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2642.4695] ep_len:[1000]\n",
      "step:[810000/1000000][81.0%] time:01:38:57.\n",
      " [Evaluate] [0/3] ep_ret:[2722.3203] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2720.8434] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2704.7436] ep_len:[1000]\n",
      "step:[820000/1000000][82.0%] time:01:40:11.\n",
      " [Evaluate] [0/3] ep_ret:[2639.5578] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2634.5031] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2635.3661] ep_len:[1000]\n",
      "step:[830000/1000000][83.0%] time:01:41:24.\n",
      " [Evaluate] [0/3] ep_ret:[2692.3821] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2697.0489] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2683.8988] ep_len:[1000]\n",
      "step:[840000/1000000][84.0%] time:01:42:38.\n",
      " [Evaluate] [0/3] ep_ret:[2687.4318] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2697.0283] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2692.6831] ep_len:[1000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:[850000/1000000][85.0%] time:01:43:52.\n",
      " [Evaluate] [0/3] ep_ret:[2796.4630] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2791.4294] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2796.5230] ep_len:[1000]\n",
      "step:[860000/1000000][86.0%] time:01:45:06.\n",
      " [Evaluate] [0/3] ep_ret:[2746.4607] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2731.9272] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2735.8504] ep_len:[1000]\n",
      "step:[870000/1000000][87.0%] time:01:46:20.\n",
      " [Evaluate] [0/3] ep_ret:[2768.4841] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2768.4501] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2772.3249] ep_len:[1000]\n",
      "step:[880000/1000000][88.0%] time:01:47:34.\n",
      " [Evaluate] [0/3] ep_ret:[2779.5493] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2768.6372] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2773.8175] ep_len:[1000]\n",
      "step:[890000/1000000][89.0%] time:01:48:47.\n",
      " [Evaluate] [0/3] ep_ret:[2797.5501] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2808.2333] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2805.1703] ep_len:[1000]\n",
      "step:[900000/1000000][90.0%] time:01:50:01.\n",
      " [Evaluate] [0/3] ep_ret:[2804.6667] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2803.1569] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2813.7339] ep_len:[1000]\n",
      "step:[910000/1000000][91.0%] time:01:51:15.\n",
      " [Evaluate] [0/3] ep_ret:[2665.0004] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2736.6202] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2736.6090] ep_len:[1000]\n",
      "step:[920000/1000000][92.0%] time:01:52:29.\n",
      " [Evaluate] [0/3] ep_ret:[2823.5913] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2816.7542] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2814.0377] ep_len:[1000]\n",
      "step:[930000/1000000][93.0%] time:01:53:43.\n",
      " [Evaluate] [0/3] ep_ret:[2763.7910] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2772.5768] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2759.5544] ep_len:[1000]\n",
      "step:[940000/1000000][94.0%] time:01:54:56.\n",
      " [Evaluate] [0/3] ep_ret:[2807.5930] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2820.3224] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2815.5885] ep_len:[1000]\n",
      "step:[950000/1000000][95.0%] time:01:56:10.\n",
      " [Evaluate] [0/3] ep_ret:[2828.1816] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2843.5785] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2528.9863] ep_len:[1000]\n",
      "step:[960000/1000000][96.0%] time:01:57:24.\n",
      " [Evaluate] [0/3] ep_ret:[2739.1076] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2755.4581] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2754.1689] ep_len:[1000]\n",
      "step:[970000/1000000][97.0%] time:01:58:38.\n",
      " [Evaluate] [0/3] ep_ret:[2820.8409] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2825.7756] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2809.9232] ep_len:[1000]\n",
      "step:[980000/1000000][98.0%] time:01:59:52.\n",
      " [Evaluate] [0/3] ep_ret:[2768.2026] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2847.7244] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2858.9346] ep_len:[1000]\n",
      "step:[990000/1000000][99.0%] time:02:01:05.\n",
      " [Evaluate] [0/3] ep_ret:[2844.6959] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2840.7464] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2873.6532] ep_len:[1000]\n",
      "step:[1000000/1000000][100.0%] time:02:02:19.\n",
      " [Evaluate] [0/3] ep_ret:[2702.9577] ep_len:[1000]\n",
      " [Evaluate] [1/3] ep_ret:[2766.4260] ep_len:[1000]\n",
      " [Evaluate] [2/3] ep_ret:[2762.7818] ep_len:[1000]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "o,ep_ret,ep_len = env.reset(),0,0\n",
    "for t in range(int(total_steps)):\n",
    "    zero_to_one = (t/total_steps)\n",
    "    one_to_zero = 1.0-zero_to_one\n",
    "    esec = time.time()-start_time\n",
    "    \n",
    "    # Get action \n",
    "    if t > start_steps: a = get_action(model,sess,o,deterministic=False)\n",
    "    else: a = env.action_space.sample()\n",
    "        \n",
    "    # Step the env\n",
    "    o2,r,d,_ = env.step(a)\n",
    "    ep_ret += r\n",
    "    ep_len += 1\n",
    "    d = False if ep_len==max_ep_len_train else d # ignore done if it maxed out \n",
    "    \n",
    "    # Store experience to replay buffers\n",
    "    replay_buffer.store(o, a, r, o2, d) # save obs, action, reward, next obs\n",
    "    replay_buffer_short.store(o, a, r, o2, d) # save obs, action, reward, next obs\n",
    "    o = o2 # easy to overlook\n",
    "    \n",
    "    # End of trajectory handling - reset env\n",
    "    if d or (ep_len == max_ep_len_train):\n",
    "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "    \n",
    "    # Update\n",
    "    if (t>=start_steps) and (t%update_every == 0):\n",
    "        for _ in range(update_count):\n",
    "            batch = replay_buffer.sample_batch(batch_size//2) \n",
    "            batch_short = replay_buffer_short.sample_batch(batch_size//2) \n",
    "            feed_dict = {model['o_ph']: np.concatenate((batch['obs1'],batch_short['obs1'])),\n",
    "                         model['o2_ph']: np.concatenate((batch['obs2'],batch_short['obs2'])),\n",
    "                         model['a_ph']: np.concatenate((batch['acts'],batch_short['acts'])),\n",
    "                         model['r_ph']: np.concatenate((batch['rews'],batch_short['rews'])),\n",
    "                         model['d_ph']: np.concatenate((batch['done'],batch_short['done']))\n",
    "                        }\n",
    "            outs = sess.run(step_ops,feed_dict=feed_dict) # train \n",
    "            q1_val,q2_val = outs[3],outs[4]\n",
    "            \n",
    "    # Evaluate\n",
    "    if (((t+1)%evaluate_every) == 0): \n",
    "        print (\"step:[%d/%d][%.1f%%] time:%s.\"%\n",
    "               (t+1,total_steps,zero_to_one*100,\n",
    "                time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time)))\n",
    "              )\n",
    "        for eval_idx in range(num_eval): \n",
    "            o,d,ep_ret,ep_len = test_env.reset(),False,0,0\n",
    "            if DO_RENDER:\n",
    "                _ = test_env.render(mode='human') \n",
    "            while not(d or (ep_len == max_ep_len_test)):\n",
    "                a = get_action(model,sess,o,deterministic=True)\n",
    "                o,r,d,_ = test_env.step(a)\n",
    "                if DO_RENDER:\n",
    "                    _ = test_env.render(mode='human') \n",
    "                ep_ret += r # compute return \n",
    "                ep_len += 1\n",
    "            print (\" [Evaluate] [%d/%d] ep_ret:[%.4f] ep_len:[%d]\"\n",
    "                %(eval_idx,num_eval,ep_ret,ep_len))\n",
    "    \n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env closed.\n"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "test_env.close()\n",
    "print (\"Env closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AntBulletEnv-v0] ready.\n",
      "[Evaluate] ep_ret:[2757.3903] ep_len:[1000]\n"
     ]
    }
   ],
   "source": [
    "gym.logger.set_level(40)\n",
    "env_name = 'AntBulletEnv-v0'\n",
    "test_env = gym.make(env_name)\n",
    "if DO_RENDER:\n",
    "    _ = test_env.render(mode='human') # enable rendering on test_env\n",
    "_ = test_env.reset()\n",
    "for _ in range(3): # dummy run for proper rendering \n",
    "    a = test_env.action_space.sample()\n",
    "    o,r,d,_ = test_env.step(a)\n",
    "    time.sleep(0.01)\n",
    "print (\"[%s] ready.\"%(env_name))\n",
    "o,d,ep_ret,ep_len = test_env.reset(),False,0,0\n",
    "if DO_RENDER:\n",
    "    _ = test_env.render(mode='human') \n",
    "while not(d or (ep_len == max_ep_len_test)):\n",
    "    a = get_action(model,sess,o,deterministic=True)\n",
    "    o,r,d,_ = test_env.step(a)\n",
    "    if DO_RENDER:\n",
    "        _ = test_env.render(mode='human') \n",
    "    ep_ret += r # compute return \n",
    "    ep_len += 1\n",
    "print (\"[Evaluate] ep_ret:[%.4f] ep_len:[%d]\"\n",
    "    %(ep_ret,ep_len))\n",
    "test_env.close() # close env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
