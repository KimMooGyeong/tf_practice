{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Restore Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:[1.15.0].\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from util import placeholders,get_mnist,suppress_tf_warning,mlp,gpu_sess,get_vars\n",
    "%matplotlib inline  \n",
    "%config InlineBackend.figure_format='retina'\n",
    "suppress_tf_warning()\n",
    "print (\"TF version:[%s].\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train:[60000], n_test:[10000], x_dim:[784], y_dim:[10]\n"
     ]
    }
   ],
   "source": [
    "x_train,y_train,x_test,y_test = get_mnist()\n",
    "n_train,n_test,x_dim,y_dim = x_train.shape[0],x_test.shape[0],\\\n",
    "    x_train.shape[1],y_train.shape[1]\n",
    "print (\"n_train:[%d], n_test:[%d], x_dim:[%d], y_dim:[%d]\"%\n",
    "       (n_train,n_test,x_dim,y_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNet Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class ConvNetClsClass(object):\n",
    "    \"\"\"\n",
    "    CNN for classification\n",
    "    \"\"\"\n",
    "    def __init__(self,name='CNN',x_dim=784,y_dim=10,img_dim=[28,28,1],\n",
    "                 filter_sizes=[32,32],kernel_sizes=[3,3],h_dims=[128],\n",
    "                 USE_BN=True,USE_DROPOUT=True):\n",
    "        self.name = name\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.img_dim = img_dim\n",
    "        \n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.h_dims = h_dims\n",
    "        \n",
    "        self.USE_BN = USE_BN\n",
    "        self.USE_DROPOUT = USE_DROPOUT\n",
    "        self.FIRST_SET_FLAG = True\n",
    "        \n",
    "        self.build_model()\n",
    "        self.build_graph()\n",
    "        print(\"[%s] instantiated.\"%(self.name))\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build model\n",
    "        \"\"\"\n",
    "        self.ph_x = tf.placeholder(dtype=tf.float32,shape=[None,self.x_dim],name='x')\n",
    "        self.ph_y = tf.placeholder(dtype=tf.float32,shape=[None,self.y_dim],name='y')\n",
    "        self.ph_is_train = tf.placeholder(tf.bool,name='is_train') \n",
    "        \n",
    "        net = tf.reshape(self.ph_x,shape=[-1]+self.img_dim) # reshape\n",
    "        \n",
    "        with tf.variable_scope('main'):\n",
    "            # Conv layers\n",
    "            for (filter_size,kernel_size) in zip(self.filter_sizes,self.kernel_sizes):\n",
    "                net = tf.layers.conv2d(inputs=net,\n",
    "                                       filters=filter_size,kernel_size=kernel_size,\n",
    "                                       padding='same',activation=None)\n",
    "                net = tf.layers.max_pooling2d(inputs=net,pool_size=2,strides=2)\n",
    "                if self.USE_BN:\n",
    "                    net = tf.layers.batch_normalization(net,training=self.ph_is_train)\n",
    "                net = tf.nn.relu(net)\n",
    "\n",
    "            # Dense layers\n",
    "            net = tf.layers.flatten(net)\n",
    "            net = mlp(net,h_dims=self.h_dims+[self.y_dim],actv=tf.nn.relu,out_actv=None,\n",
    "                      USE_DROPOUT=self.USE_DROPOUT,ph_is_training=self.ph_is_train)\n",
    "        self.y_hat = net\n",
    "        self.main_vars = get_vars('main')\n",
    "        \n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "        Build graph\n",
    "        \"\"\"\n",
    "        self.costs = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=self.ph_y,logits=self.y_hat)\n",
    "        self.cost = tf.reduce_mean(self.costs) \n",
    "        self.update_ops = tf.compat.v1.get_collection(tf.GraphKeys.UPDATE_OPS) # BN\n",
    "        self.optm = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(self.cost)\n",
    "        self.optm = tf.group([self.optm,self.update_ops])\n",
    "        self.corr = tf.equal(tf.argmax(self.y_hat,1),tf.argmax(self.ph_y,1)) # [N]\n",
    "        self.accr = tf.reduce_mean(tf.cast(self.corr, \"float\")) # [1]\n",
    "        \n",
    "    def update(self,sess,x_batch,y_batch):\n",
    "        \"\"\"\n",
    "        Update model \n",
    "        \"\"\"\n",
    "        feeds = {self.ph_x:x_batch,self.ph_y:y_batch,self.ph_is_train:True}\n",
    "        cost_val,_ = sess.run([self.cost,self.optm],feed_dict=feeds)\n",
    "        return cost_val\n",
    "    \n",
    "    def get_accr(self,sess,x,y,batch_size=256):\n",
    "        \"\"\"\n",
    "        Get accuracy\n",
    "        \"\"\"\n",
    "        n = x.shape[0] # number of data\n",
    "        accr_val_sum = 0.0\n",
    "        for it in range(np.ceil(n/batch_size).astype(np.int)):\n",
    "            x_batch = x[it*batch_size:(it+1)*batch_size,:]\n",
    "            y_batch = y[it*batch_size:(it+1)*batch_size,:]\n",
    "            feeds = {self.ph_x:x_batch,self.ph_y:y_batch,self.ph_is_train:False}\n",
    "            accr_val = sess.run(self.accr,feed_dict=feeds)\n",
    "            accr_val_sum += accr_val*x_batch.shape[0]\n",
    "        accr_val_avg = accr_val_sum/n # average out accuracy \n",
    "        return accr_val_avg\n",
    "    \n",
    "    def save(self,npz_path,sess,VERBOSE=False):\n",
    "        \"\"\"\n",
    "        Save model\n",
    "        \"\"\"\n",
    "        # Accumulate weights\n",
    "        tf_vars = self.main_vars\n",
    "        data2save,var_names,var_vals = dict(),[],[]\n",
    "        for v_idx,tf_var in enumerate(tf_vars):\n",
    "            var_name,var_val = tf_var.name,sess.run(tf_var)\n",
    "            var_names.append(var_name)\n",
    "            var_vals.append(var_val)\n",
    "            data2save[var_name] = var_val\n",
    "            if VERBOSE:\n",
    "                print (\"[%02d]  var_name:[%s]  var_shape:%s\"%\n",
    "                    (v_idx,var_name,var_val.shape,))  \n",
    "        # Create folder if not exist\n",
    "        dir_name = os.path.dirname(npz_path)\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.makedirs(dir_name)\n",
    "            print (\"[%s] created.\"%(dir_name))\n",
    "        # Save npz\n",
    "        np.savez(npz_path,**data2save)\n",
    "        print (\"[%s] saved.\"%(npz_path))\n",
    "    \n",
    "    def restore(self,npz_path,sess):\n",
    "        \"\"\"\n",
    "        Restore model\n",
    "        \"\"\"\n",
    "        # Load npz\n",
    "        l = np.load(npz_path)\n",
    "        print (\"[%s] loaded.\"%(npz_path))\n",
    "        # Get values of SAC model  \n",
    "        tf_vars = self.main_vars\n",
    "        var_vals = []\n",
    "        for tf_var in tf_vars:\n",
    "            var_vals.append(l[tf_var.name])   \n",
    "        # Set weights\n",
    "        if self.FIRST_SET_FLAG:\n",
    "            self.FIRST_SET_FLAG = False\n",
    "            self.assign_placeholders = []\n",
    "            self.assign_ops = []\n",
    "            for w_idx,weight_tf_var in enumerate(self.main_vars): \n",
    "                a = weight_tf_var\n",
    "                assign_placeholder = tf.placeholder(a.dtype, shape=a.get_shape())\n",
    "                assign_op = a.assign(assign_placeholder)\n",
    "                self.assign_placeholders.append(assign_placeholder)\n",
    "                self.assign_ops.append(assign_op)\n",
    "        for w_idx,weight_tf_var in enumerate(self.main_vars): \n",
    "            sess.run(self.assign_ops[w_idx],\n",
    "                     {self.assign_placeholders[w_idx]:var_vals[w_idx]})\n",
    "        \n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate and Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CNN] instantiated.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = gpu_sess() # open session\n",
    "C = ConvNetClsClass(name='CNN',x_dim=784,y_dim=10,img_dim=[28,28,1],\n",
    "                    filter_sizes=[32,32],kernel_sizes=[3,3],h_dims=[128],\n",
    "                    USE_BN=True,USE_DROPOUT=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[0/10] train_accuracy:[0.971] test_accuracy:[0.972]\n",
      "epoch:[1/10] train_accuracy:[0.986] test_accuracy:[0.984]\n",
      "epoch:[2/10] train_accuracy:[0.990] test_accuracy:[0.987]\n",
      "epoch:[3/10] train_accuracy:[0.992] test_accuracy:[0.989]\n",
      "epoch:[4/10] train_accuracy:[0.994] test_accuracy:[0.991]\n",
      "epoch:[5/10] train_accuracy:[0.995] test_accuracy:[0.992]\n",
      "epoch:[6/10] train_accuracy:[0.995] test_accuracy:[0.990]\n",
      "epoch:[7/10] train_accuracy:[0.997] test_accuracy:[0.992]\n",
      "epoch:[8/10] train_accuracy:[0.997] test_accuracy:[0.990]\n",
      "epoch:[9/10] train_accuracy:[0.997] test_accuracy:[0.991]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer()) # Initialize variables\n",
    "max_epoch,batch_size,print_every = 10,128,1\n",
    "max_iter = np.ceil(n_train/batch_size).astype(np.int) # number of iterations\n",
    "for epoch in range(max_epoch):\n",
    "    p_idx = np.random.permutation(n_train)\n",
    "    for it in range(max_iter):\n",
    "        b_idx = p_idx[batch_size*(it):batch_size*(it+1)]\n",
    "        x_batch,y_batch = x_train[b_idx,:],y_train[b_idx,:]\n",
    "        C.update(sess,x_batch,y_batch)\n",
    "    if ((epoch%print_every)==0) or (epoch==(max_epoch-1)):\n",
    "        train_accr_val = C.get_accr(sess,x_train,y_train)\n",
    "        test_accr_val = C.get_accr(sess,x_test,y_test)\n",
    "        print (\"epoch:[%d/%d] train_accuracy:[%.3f] test_accuracy:[%.3f]\"%\n",
    "               (epoch,max_epoch,train_accr_val,test_accr_val))\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[../net/cnn_mnist/net.npz] saved.\n"
     ]
    }
   ],
   "source": [
    "C.save(npz_path='../net/cnn_mnist/net.npz',sess=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-init weights and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accr_val:[0.083]\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer()) \n",
    "test_accr_val = C.get_accr(sess,x_test,y_test)\n",
    "print (\"test_accr_val:[%.3f]\"%(test_accr_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[../net/cnn_mnist/net.npz] loaded.\n"
     ]
    }
   ],
   "source": [
    "C.restore(npz_path='../net/cnn_mnist/net.npz',sess=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_accr_val:[0.991]\n"
     ]
    }
   ],
   "source": [
    "test_accr_val = C.get_accr(sess,x_test,y_test)\n",
    "print (\"test_accr_val:[%.3f]\"%(test_accr_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
