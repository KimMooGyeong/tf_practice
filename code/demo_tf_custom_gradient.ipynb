{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custrom forward and backward path in TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import math, random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from util import suppress_tf_warning\n",
    "from tensorflow.python.framework import ops\n",
    "suppress_tf_warning()\n",
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"custom_gradient.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_ref:\n",
      " [[0.5488135  0.71518937 0.60276338]\n",
      " [0.54488318 0.4236548  0.64589411]\n",
      " [0.43758721 0.891773   0.96366276]] \n",
      "b_ref:\n",
      " [[0.38344152]\n",
      " [0.79172504]\n",
      " [0.52889492]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed=0)\n",
    "W_ref,b_ref = np.random.rand(3,3),np.random.rand(3,1)\n",
    "print('W_ref:\\n',W_ref,'\\nb_ref:\\n',b_ref)\n",
    "def f_unit(z): # f: z[3 x 1] -> x[3 x 1]\n",
    "    x = np.matmul(W_ref.T,np.reshape(z,newshape=(3,1)))+b_ref # [3x1] column vector\n",
    "    return x \n",
    "def g_unit(x): # g: x[3 x 1] -> y[2 x 1]\n",
    "    x = np.reshape(x,newshape=(3,1))\n",
    "    y = np.zeros(shape=(2,1))\n",
    "    x0,x1,x2 = x[0,0],x[1,0],x[2,0]\n",
    "    y[0,0] = +1*(x0**1) - 2*(x1**2) + 3*(x2**3)\n",
    "    y[1,0] = -4*(x0**2) + 5*(x1**3) - 6*(x2**1)\n",
    "    return y # [2x1] column vector\n",
    "def g_grad_unit(x): # compute Jacobian x: [3]\n",
    "    dy0_dx,dy1_dx = np.zeros(shape=(3)),np.zeros(shape=(3))\n",
    "    dy0_dx[0] = +1\n",
    "    dy0_dx[1] = -4*(x[1])\n",
    "    dy0_dx[2] = +9*(x[2]**2)\n",
    "    dy1_dx[0] = -8*(x[0])\n",
    "    dy1_dx[1] = +15*(x[1]**2)\n",
    "    dy1_dx[2] = -6\n",
    "    return dy0_dx,dy1_dx # dy0_dx: [3], dy1_dx: [3]\n",
    "def fg_batch(z): # z[n x 3] -> x[n x 3] -> y[n x 2]\n",
    "    n = z.shape[0]\n",
    "    y = np.zeros(shape=(n,2))\n",
    "    for i in range(n):\n",
    "        z_i = z[i,:]\n",
    "        x_i = f_unit(z_i)\n",
    "        y[i,:] = g_unit(x_i).T\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "def custom_g_func(x): # x -> y\n",
    "    n = x.shape[0]\n",
    "    y0,y1 = np.zeros(shape=(n,1)),np.zeros(shape=(n,1))\n",
    "    for i in range(n):\n",
    "        x_i= x[i,:]\n",
    "        y_i = g_unit(x_i) # g unit custom function \n",
    "        y0[i,0],y1[i,0] = y_i[0,0],y_i[1,0] \n",
    "    return [y0.astype(np.float32),y1.astype(np.float32)]\n",
    "\n",
    "def custom_g_derv(x,grads0,grads1): # x -> dy\n",
    "    n = x.shape[0]\n",
    "    dy0,dy1 = np.zeros(shape=(n,3)),np.zeros(shape=(n,3))\n",
    "    for i in range(n):\n",
    "        x_i = x[i,:]\n",
    "        dy0_dx,dy1_dx = g_grad_unit(x_i)\n",
    "        dy0[i,:] = grads0[i,0]*dy0_dx\n",
    "        dy1[i,:] = grads1[i,0]*dy1_dx\n",
    "    return [dy0.astype(np.float32),dy1.astype(np.float32)]\n",
    "\n",
    "def grad_wrapper(op, grads0, grads1):\n",
    "    x,out = op.inputs[0],op.outputs[0]\n",
    "    temp = tf.py_func(func=custom_g_derv,inp=[x,grads0,grads1],\n",
    "                      Tout=[tf.float32,tf.float32]) \n",
    "    dy0 = temp[0] \n",
    "    dy1 = temp[1]\n",
    "    return dy0 + dy1\n",
    "\n",
    "def py_func_wrapper(func, inp, Tout, stateful=True, name=None, grad=None):\n",
    "    rnd_name = 'custom_gradient_sj' # gradient name (make sure to be unique)\n",
    "    tf.RegisterGradient(rnd_name)(grad)\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({\"PyFunc\": rnd_name, \"PyFuncStateless\": rnd_name}):\n",
    "        return tf.py_func(func, inp, Tout, stateful=stateful, name=name)\n",
    "    \n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 1000\n",
    "z_train = -3+6*np.random.rand(n_train,3) # [10000 x 3]\n",
    "y_train = fg_batch(z_train) # [10000 x 2]\n",
    "n_test = 100\n",
    "z_test = -3+6*np.random.rand(n_test,3) # [100 x 3]\n",
    "y_test = fg_batch(z_test) # [100 x 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready.\n"
     ]
    }
   ],
   "source": [
    "init = tf.random_normal_initializer(stddev=0.1)\n",
    "W,b = tf.Variable(init(shape=[3,3])),tf.Variable(init(shape=[3]))\n",
    "z_ph = tf.placeholder(tf.float32,shape=(None,3)) # input z\n",
    "y_ph = tf.placeholder(tf.float32,shape=(None,2)) # target y\n",
    "x_pred = tf.matmul(z_ph,W) + b # f: z->x\n",
    "y_preds = py_func_wrapper(func=custom_g_func, # function\n",
    "                          inp=[x_pred], # input\n",
    "                          Tout=[tf.float32,tf.float32], # output type of 'custom_func'\n",
    "                          name='custom_g_func',\n",
    "                          grad=grad_wrapper)\n",
    "y_pred_concat= tf.concat((y_preds[0],y_preds[1]),axis=1)\n",
    "cost = tf.reduce_mean(tf.reduce_mean(tf.square(y_ph - y_pred_concat)))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost)\n",
    "print (\"Ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:[1/10000] cost:[1.294e+04] \n",
      "W_val:\n",
      " [[ 0.13522008  0.18572854  0.00611573]\n",
      " [ 0.01448664  0.16701524  0.04245107]\n",
      " [-0.0743758   0.08850946  0.03756813]] \n",
      "b_val:\n",
      " [ 0.08928414 -0.04453174 -0.07976732]\n",
      "W_ref:\n",
      " [[0.5488135  0.71518937 0.60276338]\n",
      " [0.54488318 0.4236548  0.64589411]\n",
      " [0.43758721 0.891773   0.96366276]] \n",
      "b_ref:\n",
      " [[0.38344152 0.79172504 0.52889492]]\n",
      "Iter:[100/10000] cost:[1.233e+04] \n",
      "Iter:[200/10000] cost:[1.081e+04] \n",
      "Iter:[300/10000] cost:[8.584e+03] \n",
      "Iter:[400/10000] cost:[6.929e+03] \n",
      "Iter:[500/10000] cost:[6.248e+03] \n",
      "Iter:[600/10000] cost:[5.876e+03] \n",
      "Iter:[700/10000] cost:[5.617e+03] \n",
      "Iter:[800/10000] cost:[5.444e+03] \n",
      "Iter:[900/10000] cost:[5.225e+03] \n",
      "Iter:[1000/10000] cost:[3.421e+03] \n",
      "W_val:\n",
      " [[0.26110274 0.7488662  0.39169502]\n",
      " [0.38215727 0.4436264  0.7469938 ]\n",
      " [0.13420638 0.8218259  0.2869    ]] \n",
      "b_val:\n",
      " [ 0.04189656  0.60155076 -0.09727084]\n",
      "W_ref:\n",
      " [[0.5488135  0.71518937 0.60276338]\n",
      " [0.54488318 0.4236548  0.64589411]\n",
      " [0.43758721 0.891773   0.96366276]] \n",
      "b_ref:\n",
      " [[0.38344152 0.79172504 0.52889492]]\n",
      "Iter:[1100/10000] cost:[1.265e+03] \n",
      "Iter:[1200/10000] cost:[7.063e+02] \n",
      "Iter:[1300/10000] cost:[3.740e+02] \n",
      "Iter:[1400/10000] cost:[2.063e+02] \n",
      "Iter:[1500/10000] cost:[1.027e+02] \n",
      "Iter:[1600/10000] cost:[5.242e+01] \n",
      "Iter:[1700/10000] cost:[2.686e+01] \n",
      "Iter:[1800/10000] cost:[1.362e+01] \n",
      "Iter:[1900/10000] cost:[7.201e+00] \n",
      "Iter:[2000/10000] cost:[4.337e+00] \n",
      "W_val:\n",
      " [[0.37934342 0.71910584 0.609006  ]\n",
      " [0.4258198  0.41682237 0.65844655]\n",
      " [0.12117734 0.8944142  0.9455714 ]] \n",
      "b_val:\n",
      " [0.1119029 0.7011867 0.5340755]\n",
      "W_ref:\n",
      " [[0.5488135  0.71518937 0.60276338]\n",
      " [0.54488318 0.4236548  0.64589411]\n",
      " [0.43758721 0.891773   0.96366276]] \n",
      "b_ref:\n",
      " [[0.38344152 0.79172504 0.52889492]]\n",
      "Iter:[2100/10000] cost:[2.874e+00] \n",
      "Iter:[2200/10000] cost:[2.132e+00] \n",
      "Iter:[2300/10000] cost:[1.715e+00] \n",
      "Iter:[2400/10000] cost:[1.476e+00] \n",
      "Iter:[2500/10000] cost:[1.268e+00] \n",
      "Iter:[2600/10000] cost:[1.131e+00] \n",
      "Iter:[2700/10000] cost:[9.882e-01] \n",
      "Iter:[2800/10000] cost:[8.926e-01] \n",
      "Iter:[2900/10000] cost:[8.123e-01] \n",
      "Iter:[3000/10000] cost:[7.460e-01] \n",
      "W_val:\n",
      " [[0.39542195 0.7138322  0.6026128 ]\n",
      " [0.49795792 0.41836998 0.6451775 ]\n",
      " [0.2261728  0.89670295 0.9638289 ]] \n",
      "b_val:\n",
      " [0.15707606 0.72481126 0.5288029 ]\n",
      "W_ref:\n",
      " [[0.5488135  0.71518937 0.60276338]\n",
      " [0.54488318 0.4236548  0.64589411]\n",
      " [0.43758721 0.891773   0.96366276]] \n",
      "b_ref:\n",
      " [[0.38344152 0.79172504 0.52889492]]\n",
      "Iter:[3100/10000] cost:[6.743e-01] \n",
      "Iter:[3200/10000] cost:[6.089e-01] \n",
      "Iter:[3300/10000] cost:[5.436e-01] \n",
      "Iter:[3400/10000] cost:[5.046e-01] \n",
      "Iter:[3500/10000] cost:[4.564e-01] \n",
      "Iter:[3600/10000] cost:[4.108e-01] \n",
      "Iter:[3700/10000] cost:[3.835e-01] \n",
      "Iter:[3800/10000] cost:[3.417e-01] \n",
      "Iter:[3900/10000] cost:[3.153e-01] \n",
      "Iter:[4000/10000] cost:[2.678e-01] \n",
      "W_val:\n",
      " [[0.44221738 0.7140159  0.6025904 ]\n",
      " [0.5179672  0.4207785  0.6453593 ]\n",
      " [0.29009616 0.8945722  0.96377575]] \n",
      "b_val:\n",
      " [0.22937171 0.74365526 0.5287192 ]\n",
      "W_ref:\n",
      " [[0.5488135  0.71518937 0.60276338]\n",
      " [0.54488318 0.4236548  0.64589411]\n",
      " [0.43758721 0.891773   0.96366276]] \n",
      "b_ref:\n",
      " [[0.38344152 0.79172504 0.52889492]]\n",
      "Iter:[4100/10000] cost:[2.437e-01] \n",
      "Iter:[4200/10000] cost:[2.213e-01] \n",
      "Iter:[4300/10000] cost:[2.078e-01] \n",
      "Iter:[4400/10000] cost:[1.797e-01] \n",
      "Iter:[4500/10000] cost:[1.649e-01] \n",
      "Iter:[4600/10000] cost:[1.455e-01] \n",
      "Iter:[4700/10000] cost:[1.337e-01] \n",
      "Iter:[4800/10000] cost:[1.245e-01] \n",
      "Iter:[4900/10000] cost:[1.076e-01] \n",
      "Iter:[5000/10000] cost:[9.447e-02] \n",
      "W_val:\n",
      " [[0.4804077  0.71442145 0.60266066]\n",
      " [0.5274878  0.42185125 0.6455817 ]\n",
      " [0.3443105  0.893452   0.96365434]] \n",
      "b_val:\n",
      " [0.28727162 0.76037544 0.52880317]\n",
      "W_ref:\n",
      " [[0.5488135  0.71518937 0.60276338]\n",
      " [0.54488318 0.4236548  0.64589411]\n",
      " [0.43758721 0.891773   0.96366276]] \n",
      "b_ref:\n",
      " [[0.38344152 0.79172504 0.52889492]]\n",
      "Iter:[5100/10000] cost:[8.173e-02] \n",
      "Iter:[5200/10000] cost:[7.416e-02] \n",
      "Iter:[5300/10000] cost:[6.566e-02] \n",
      "Iter:[5400/10000] cost:[5.671e-02] \n",
      "Iter:[5500/10000] cost:[5.046e-02] \n",
      "Iter:[5600/10000] cost:[4.372e-02] \n",
      "Iter:[5700/10000] cost:[3.858e-02] \n",
      "Iter:[5800/10000] cost:[3.373e-02] \n",
      "Iter:[5900/10000] cost:[2.999e-02] \n",
      "Iter:[6000/10000] cost:[2.580e-02] \n",
      "W_val:\n",
      " [[0.51214015 0.7147804  0.602685  ]\n",
      " [0.5355855  0.4227061  0.6456946 ]\n",
      " [0.38616756 0.89268106 0.9636584 ]] \n",
      "b_val:\n",
      " [0.3303576  0.77390224 0.52883434]\n",
      "W_ref:\n",
      " [[0.5488135  0.71518937 0.60276338]\n",
      " [0.54488318 0.4236548  0.64589411]\n",
      " [0.43758721 0.891773   0.96366276]] \n",
      "b_ref:\n",
      " [[0.38344152 0.79172504 0.52889492]]\n",
      "Iter:[6100/10000] cost:[2.167e-02] \n",
      "Iter:[6200/10000] cost:[1.834e-02] \n",
      "Iter:[6300/10000] cost:[1.537e-02] \n",
      "Iter:[6400/10000] cost:[1.378e-02] \n",
      "Iter:[6500/10000] cost:[1.058e-02] \n",
      "Iter:[6600/10000] cost:[1.008e-02] \n",
      "Iter:[6700/10000] cost:[7.524e-03] \n",
      "Iter:[6800/10000] cost:[7.066e-03] \n",
      "Iter:[6900/10000] cost:[5.223e-03] \n",
      "Iter:[7000/10000] cost:[4.392e-03] \n",
      "W_val:\n",
      " [[0.5328611  0.71502984 0.60273117]\n",
      " [0.5409514  0.42330065 0.64582384]\n",
      " [0.41538003 0.8921229  0.96364474]] \n",
      "b_val:\n",
      " [0.36061814 0.78379804 0.5288713 ]\n",
      "W_ref:\n",
      " [[0.5488135  0.71518937 0.60276338]\n",
      " [0.54488318 0.4236548  0.64589411]\n",
      " [0.43758721 0.891773   0.96366276]] \n",
      "b_ref:\n",
      " [[0.38344152 0.79172504 0.52889492]]\n",
      "Iter:[7100/10000] cost:[3.618e-03] \n",
      "Iter:[7200/10000] cost:[2.709e-03] \n",
      "Iter:[7300/10000] cost:[2.240e-03] \n",
      "Iter:[7400/10000] cost:[1.813e-03] \n",
      "Iter:[7500/10000] cost:[1.407e-03] \n",
      "Iter:[7600/10000] cost:[1.059e-03] \n",
      "Iter:[7700/10000] cost:[9.486e-04] \n",
      "Iter:[7800/10000] cost:[6.348e-04] \n",
      "Iter:[7900/10000] cost:[4.835e-04] \n",
      "Iter:[8000/10000] cost:[3.908e-04] \n",
      "W_val:\n",
      " [[0.54430205 0.7151395  0.6027554 ]\n",
      " [0.5436307  0.4235459  0.6458686 ]\n",
      " [0.4312117  0.89187837 0.9636601 ]] \n",
      "b_val:\n",
      " [0.37660167 0.7893771  0.528883  ]\n",
      "W_ref:\n",
      " [[0.5488135  0.71518937 0.60276338]\n",
      " [0.54488318 0.4236548  0.64589411]\n",
      " [0.43758721 0.891773   0.96366276]] \n",
      "b_ref:\n",
      " [[0.38344152 0.79172504 0.52889492]]\n",
      "Iter:[8100/10000] cost:[2.575e-04] \n",
      "Iter:[8200/10000] cost:[1.988e-04] \n",
      "Iter:[8300/10000] cost:[1.356e-04] \n",
      "Iter:[8400/10000] cost:[9.727e-05] \n",
      "Iter:[8500/10000] cost:[7.021e-05] \n",
      "Iter:[8600/10000] cost:[4.569e-05] \n",
      "Iter:[8700/10000] cost:[2.795e-05] \n",
      "Iter:[8800/10000] cost:[1.871e-05] \n",
      "Iter:[8900/10000] cost:[1.298e-05] \n",
      "Iter:[9000/10000] cost:[7.442e-06] \n",
      "W_val:\n",
      " [[0.54814386 0.71518266 0.60276145]\n",
      " [0.54471457 0.42363963 0.64589036]\n",
      " [0.43665093 0.8917856  0.96366036]] \n",
      "b_val:\n",
      " [0.3824863  0.7913873  0.52889276]\n",
      "W_ref:\n",
      " [[0.5488135  0.71518937 0.60276338]\n",
      " [0.54488318 0.4236548  0.64589411]\n",
      " [0.43758721 0.891773   0.96366276]] \n",
      "b_ref:\n",
      " [[0.38344152 0.79172504 0.52889492]]\n",
      "Iter:[9100/10000] cost:[5.377e-06] \n",
      "Iter:[9200/10000] cost:[2.970e-06] \n",
      "Iter:[9300/10000] cost:[1.585e-06] \n",
      "Iter:[9400/10000] cost:[7.782e-07] \n",
      "Iter:[9500/10000] cost:[4.990e-07] \n",
      "Iter:[9600/10000] cost:[2.391e-07] \n",
      "Iter:[9700/10000] cost:[1.438e-07] \n",
      "Iter:[9800/10000] cost:[5.750e-08] \n",
      "Iter:[9900/10000] cost:[3.257e-08] \n",
      "Iter:[10000/10000] cost:[2.520e-08] \n",
      "W_val:\n",
      " [[0.5487845  0.7151891  0.6027633 ]\n",
      " [0.54487854 0.42365417 0.645894  ]\n",
      " [0.43754742 0.8917737  0.9636628 ]] \n",
      "b_val:\n",
      " [0.3834034 0.7917115 0.5288948]\n",
      "W_ref:\n",
      " [[0.5488135  0.71518937 0.60276338]\n",
      " [0.54488318 0.4236548  0.64589411]\n",
      " [0.43758721 0.891773   0.96366276]] \n",
      "b_ref:\n",
      " [[0.38344152 0.79172504 0.52889492]]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "max_iter = 10000\n",
    "for it in range(max_iter):\n",
    "    batch_idx = np.random.permutation(n_train)[:32]\n",
    "    z_batch,y_batch = z_train[batch_idx,:],y_train[batch_idx,:]\n",
    "    cost_val,_,y_pred_concat_val,W_val,b_val = sess.run([cost,optimizer,y_pred_concat,W,b], \n",
    "                                                        feed_dict={z_ph: z_batch, y_ph: y_batch})\n",
    "    if (it==0) or (((it+1)%100)==0):\n",
    "        cost_val = sess.run(cost,feed_dict={z_ph:z_test,y_ph:y_test})\n",
    "        print (\"Iter:[%d/%d] cost:[%.3e] \"% (it+1,max_iter,cost_val,))\n",
    "    if (it==0) or (((it+1)%1000)==0):\n",
    "        print ('W_val:\\n',W_val,'\\nb_val:\\n',b_val)\n",
    "        print ('W_ref:\\n',W_ref,'\\nb_ref:\\n',b_ref.T)\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
